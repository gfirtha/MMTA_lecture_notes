The previous chapter introduced the basic properties of consumer and professional, studio video parameters.

The active spatial resolution and the resulting bit rates of frequently used digital video formats are summarized in Table  \ref{tab:bitrate_2}.
%
\begin{table}[h!]
\caption{The active bitrate of frequently used video formats along with the size, required for storing 1 hour of video stream}
\renewcommand*{\arraystretch}{2}
\label{tab:bitrate_2}
\begin{center}
    \begin{tabular}[h!]{ @{}c | l | l | l | l | l @{} }%\toprule
\thead{Format} & \thead{Active\\ resolution} & \thead{Active bitrate\\4:2:2} & \thead{Active bitrate\\4:2:0}& \thead{Size of 1 hour\\ video} \\ \hline
SIF ($i59.54$)    & $352\times 240$ &   $40.6~\mathrm{Mbit/s}$  & $30.4~\mathrm{Mbit/s}$  & $13.7~\mathrm{Gbyte}$ \\
CIF ($i59.54$)   &  $352 \times 288$ & $48.6~\mathrm{Mbit/s}$   & $36.5~\mathrm{Mbit/s}$  & $16.4~\mathrm{Gbyte}$ \\
$576i50$    &  $576\times 720$  &   $199~\mathrm{Mbit/s}$       & $149.1~\mathrm{Mbit/s}$ & $67.1~\mathrm{Gbyte}$ \\
$720p60$   &  $1280\times 720$   &   $883~\mathrm{Mbit/s}$    	& $662.8~\mathrm{Mbit/s}$  & $298.3~\mathrm{Gbyte}$ \\
$1080i30$ 	&  $1920\times 1080$  &   $994~\mathrm{Mbit/s}$    	& $745.8~\mathrm{Mbit/s}$  & $335.6~\mathrm{Gbyte}$ \\
$1080p60$ 	 &  $1920\times 1080$ &   $1.99~\mathrm{Gbit/s}$    & $1.49~\mathrm{Gbit/s}$  & $671.2~\mathrm{Gbyte}$ \\
$2160p60$ (10 bits)	&  $3840\times 2160$  &   $9.95~\mathrm{Gbit/s}$   &  $7.47~\mathrm{Gbit/s}$& $3.36~\mathrm{Tbyte}$ \\
$4320p60$ (10 bits)	&  $7680 \times 4320$  &   $39.8~\mathrm{Gbit/s}$   &  $29.9~\mathrm{Gbit/s}$& $13.44~\mathrm{Tbyte}$ 
\end{tabular}
\end{center}
\end{table}
%

In the table SIF and CIF abbreviate Source Input Format and Common Intermediate Format respectively.
Both formats were introduced for the consumer digital representation of NTSC and PAL videos---with CIF being the default video format of the H.261 encoder and SIF being that for the MPEG-1 standard--- with a halved vertical resolution when compared to the professional ITU-601 studio standard.
	
As the table verifies it, the generated data rate of video formats---and thus the required storage space---grows exponentially with higher spatial and temporal resolution.
Modern studio and consumer interfaces---variants of the SDI interface for studio applications and HDMI or DisplayPort for consumer use---allow the transmission of the data rates of uncompressed video over short ranges, e.g. between local devices.
However, the storage and broadcasting of such high data rates is virtually impossible:
the compression of digital video data is indispensable.

\vspace{3mm}
Fortunately, real-life sequence of images contain significant amount of redundant information:
Statistically speaking within single frames the neighboring pixels are usually highly correlated.
Similarly, consequent frames are usually very similar to each other, even if they contain objects under motion.
In video signals, the redundancy can be classified as spatial, temporal, coding and psychovisual redundancies:
\begin{itemize}
\item Spatial redundancy (or intraframe/interpixel redundancy) is present in areas of images or video frames where pixel values vary only by small amounts.
\item Temporal redundancy (or interframe redundancy) is present in video signals when there is significant similarity between successive video frames.
\item Coding Redundancy is present if the symbols produced by the video encoder are inefficiently mapped to a binary bitstream. Typically, entropy coding techniques can be used in order to exploit the statistics of the output video data where some symbols occur with greater probability than others.
\item Psychovisual redundancy is present either in a video signal or a still image containing perceptually unimportant information:
The eye and the brain do not respond to all visual information with same sensitivity, some information is neglected during the processing by the brain. 
Elimination of this information does not affect the interpretation of the image by the brain and may lead to a significant compression.
Psychovisual redundancy is usually removed by appropriate requantization of the video data, so that the quantization noise remains under the threshold of visibility.
\end{itemize}
In order to achieve a high compression ratio, all the above redundancy types should be eliminated, being the basic goal of a 
\textbf{source encoder}.

%\begin{figure}[]
%	\centering
%	\begin{overpic}[width = 0.8\columnwidth ]{figures_en/digit_channel_modell.png}
%	\end{overpic}
%	\caption{Az azonos alapszínekkel dolgozó SD formátum, HD formátum és az sRGB színtér gamutja $xy$ és $uv$ diagramon ábrázolva.}
%	\label{Fig:digit_channel}
%\end{figure}
%Figure \ref{Fig:digit_channel} illustrates the general model of the digital video transmission channel.
%

\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.8\columnwidth ]{figures_en/source_encoder.png}
	\end{overpic}
	\caption{Block scheme of a general video/audio source encoder.}
	\label{Fig:source_encoder}
\end{figure}
Generally speaking, the aim of source encoding is reducing the source redundancy by keeping only the relevant information, based on the properties of the source and the sink.
The source in this case is the video (or possibly audio) sequence, and the sink is the human visual system (or the auditory system for audio info).
The general structure of a source encoder, valid both for video or audio inputs is depicted in Figure \ref{Fig:source_encoder}.
The reduction of the different types of redundancy is performed by the following steps:
\begin{itemize}
\item Change of representation: in order to reduce spatial and temporal the input data is represented in a new data space containing less redundancy.
The change of representation can be performed by 
	\begin{itemize}
	\item Differential coding (DPCM: Differential Pulse Code Modulation)
	\item Transformation coding
	\item Sub-band coding
	\end{itemize}
\item Irreversible coding: the accuracy of representation is reduced by removing irrelevant information, hence, eliminating psychovisual redundancy.
Irreversible coding is achieved by 
	\begin{itemize}
	\item requantization of the data
	\item spatial and temporal subsampling
	\end{itemize}
\item Reversible coding: an efficient code-assignment is established reducing statistical redundancy.
Types of reversible entropy coding applied often in video, image and audio processing are
	\begin{itemize}
	\item Variable Length Coding (VLC)
	\item Run-Length Coding (RLC)
	\end{itemize}
\end{itemize}

In the following this chapter introduces the basic concepts of compression methods, based on differential coding and transformation coding.
The basic concepts are introduced for the generalized case of arbitrary one and two dimensional input signals, and later specialized to video signal inputs.

\section{Predictive coding}

Predictive coding, or differential quantization is a compression technique, utilizing linear prediction along with the requantization of the predicted data (i.e. performing both a change of representation and irreversible coding):
instead of the direct quantization and transmission of the input signal, the actual input sample is predicted with an appropriately chosen prediction algorithm, and only the discrepancy between the actual and the estimated sample is further processed.
In the receiver the same prediction is performed as in the source side, and the output sample is obtained as the sum of the estimated signal and the error of estimation. 

The signal processing steps in a differential encoder and decoder are shown in Figure \ref{Fig:diff_quant} with the following notation:
\begin{itemize}
\item $\xi(n)$ is the input source sample
\item $\hat{\xi}(n)$ is the predicted input sample
\item $\delta(n)$ is the error of prediction/differential signal
\item $Q$ is the quantization of the signal
\item $Q^{-1}$ is the inverse quantization
\item $\delta'(n)$ is the quantized differential signal
\item $\xi'(n)$ is the quantized, reconstructed input sample
\end{itemize}
In the block diagram quantization is performed by rescaling the input signal to match the dynamic range of the quantizer, followed by the rounding of the signal level to the nearest integer.
Inverse quantizer, on the other hand scales back the quantized signal to the original dynamic range (obviously, information loss can not be reversed).

\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.6\columnwidth ]{figures_en/diff_quant.png}
	\end{overpic}
	\caption{Block scheme of a general differential encoder and decoder.}
	\label{Fig:diff_quant}
\end{figure}

The basic idea behind differential quantization is the following:
Assuming an efficient prediction the dynamic range of the differential signal is significantly smaller than that of the original input signal.
Therefore, discretizing the error signal means the division of a smaller dynamic range to the same number of intervals ($2^N$ in case of $N$ bits representation) than in case of quantizing the input signal directly, resulting in an increased resolution, or mathematically speaking, in an increased signal-to-noise ratio.
Alternatively, the same signal-to-noise ratio may be achieved by using lower bit depths utilizing differential quantization.

\vspace{3mm}
In order to give a mathematical description on differential quantization and quantify the introduced quantities, first a brief summary of stochastic processes is given.

\subsection{Basic stochastic concepts}

A stochastic process is any process describing the evolution in time or space of a random phenomenon, given by an indexed sequence of random samples.
Each sample is a random variable with a given probability distribution, and with the probability usually depending on the previous samples.
For the sake of simplicity it is implied here that the process evolves over time, but all the following can be easily extended for e.g. spatially dependent processes.

Let $\xi$ denote a stochastic process, and the sample index denoted by $n$, hence for each index $\xi(n)$ is a random variable.
A stochastic process is fully described by its joint distribution function,  which is, however, rarely available either by measurement or analytically.
Instead, more often stochastic processes are characterized in a simplified manner by their \textbf{moments} (being the \textbf{mean value} its first and the \textbf{variance} its second moment) and the \textbf{autocorrelation function}.

\paragraph*{Wide-sense stationary processes:}
In the following only \textbf{stationary processes} are investigated, that's statistical properties do not change over time.
Strict stationary requires the entire joint distribution function of the process to be time invariant.
In most applications it is sufficient to require the process to be \textbf{wide-sense stationary (WSS)}, defined by the following properties:
\begin{itemize}
\item The mean/expected value of a WSS process is constant, invariant of $n$:
\begin{equation}
m_\xi(n) = m_\xi
\end{equation}
Once the above relation holds, the expected values of the process can be approximated as the average of a realization of length $N$ according to
\begin{equation}
m_\xi = \EX(\xi(n) ) = \frac{1}{N} \sum_{n = 1}^{N} \xi(n)
\end{equation}
\item For a general process the autocorrelation function can be defined for two distinct samples, i.e. it is a two-dimensional function 
\begin{equation}
r_\xi(n_1,n_2) = \EX( \xi(n_1) \cdot \xi(n_2) ),
\end{equation}
loosely speaking measuring the linear dependence between samples $\xi(n_1)$ and $\xi(n_2)$.
If two samples are uncorrelated---i.e. $r_\xi(n_1,n_2)=0$---it implies that no linear relation exists between them, however, higher order dependence may be present.
Therefore, uncorrelatedness does not imply independence (while independence strictly ensures uncorrelatednes).

For a WSS process this linear dependence is translation invariant
\begin{equation}
r_\xi(n_1,n_2) = r_\xi(n_1+ d , n_2 + d), \hspace{1cm} \forall d \in \mathcal{N}
\end{equation}
therefore autocorrelation depends only on the distance of the two samples (denoted now by $d$)
\begin{equation}
r_\xi(n_1 - n_2) = r_\xi(d).
\end{equation}
If the above relation holds, autocorrelation can be statistically approximated from a realization of the process as
\begin{equation}
r_\xi(d) = \EX( \xi(n) \cdot \tilde{\xi}(n + d) ) = \frac{1}{N} \sum_{n = 0}^N \xi(n) \xi(n + d)
\label{Eq:correlation_Def}
\end{equation}
\item As a further property for WSS process the auto-correlation function at zero lag ($d=0$) gives the mean value of the squared samples, i.e. the mean energy of the process, being obviously also time invariant
\begin{equation}
r_\xi(0) = E_\xi =  \EX( \xi(n)^2 ) = \frac{1}{N} \sum_{n = 0}^N \xi(n)^2.
\end{equation}
\end{itemize}

\paragraph*{Noise processes:}
As the most simple stochastic example, an uncorrelated random process is considered, meaning that linear relation exists between neighboring samples.
For such a process the autocorrelation is zero valued everywhere, except for zero lag ($d=0$), where the autocorrelation value is the energy of the random process.
The autocorrelation, therefore, is a Kronecker delta (discrete Dirac delta) function at the origin, given by
\begin{equation}
r_\xi(n) = E_\xi  \cdot \delta(n) = \begin{cases} 0, \hspace{7mm} \text{if} \hspace{2mm} n = 0 \\ E_\xi , \hspace{7mm} \text{elsewhere.} \end{cases} 
\end{equation}
Such a stochastic process is called \textbf{white noise}.
The distribution of the individual samples is arbitrary, most often the samples are drawn from uniform or Gaussian normal distribution.

\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.85\columnwidth ]{figures_en/white_noise.png}
	\small
	\put(0,0){(a)}
	\end{overpic}
	\begin{overpic}[width = 0.85\columnwidth ]{figures_en/white_noise2.png}
	\small
	\put(0,0){(b)}
	\end{overpic}
	\caption{One dimensional and two dimensional white noise process (a) and correlated noise process (b).}
	\label{Fig:noise}
\end{figure}
The terminology originates from the \textbf{power spectral density}, defined as the Fourier transform of the autocorrelation function , describing the frequency content of the stochastic process.	
For white noise the spectral density function is constant, similarly to the spectrum of white light containing all lights with all the visible wavelengths equally.
A simple example realization of white noise process is depicted in Figure \ref{Fig:noise} (a) in one and two dimensions.

A correlated process can be most easily generated from white noise by linear filtering (e.g. FIR filtering):
since after filtering each output sample is produced as the linear combination of the previous samples, therefore neighboring samples become correlated, and the autocorrelation is described by the filtering coefficients themselves.
Correlated noise, obtained by filtering of the exemplary white noise realization is depicted in Figure \ref{Fig:noise}.

\subsection{The goal of differential quantization}

Having introduced basic stochastic concepts differential quantization can be discussed mathematically.

In the model applied the input signal $\xi(n)$ is assumed to be a wide sense stationary process.
The effect of quantization can be most easily modeled as an additive noise $\epsilon(n)$, added to the quantized signal.
Efficiency of quantization is usually described by the signal-to-quantization-noise ratio, defined as the ratio of the energy of the quantized signal and the quantization noise, written as
\begin{equation}
\mathrm{SQNR} = \frac{\EX(\xi(n)^2)}{\EX(\epsilon(n)^2)},
\end{equation}
assuming that the quantized signal is the input signal directly.

In an ideal case where the quantization error is uniformly distributed and the signal has a uniform distribution covering all quantization levels the quantization noise can be calculated as
\begin{equation}
\mathrm{SQNR} = 20 \mathrm{log}_{10} 2^N,
\end{equation}
where $N$ is the bit depth.
In case that differential quantization is applied, two statements can be made
\begin{itemize}
\item Assuming that in the receiver side the input signal can be regenerated from the quantized differential signal the final signal-to-noise ratio can be calculated as 
\begin{equation}
\mathrm{SNR} = \frac{\EX(\tilde{\xi}(n)^2)}{\EX(\epsilon(n)^2)},
\end{equation}
\item However, instead of the input signal, the differential signal is quantized, setting the quantization SNR to
\begin{equation}
\mathrm{SQNR} = \frac{\EX(\delta(n)^2)}{\EX(\epsilon(n)^2)} = 20 \mathrm{log}_{10} 2^N.
\end{equation}
\end{itemize}
Rewriting the above equations results in the total SNR of 
\begin{equation}
\mathrm{SNR} = \frac{\EX(\tilde{\xi}(n)^2)}{\EX(\epsilon(n)^2)} = \frac{\EX(\tilde{\xi}(n)^2)}{\EX(\delta(n)^2)} \cdot 
\underbrace{\frac{\EX(\delta(n)^2)}{\EX(\epsilon(n)^2)}}_{20 \mathrm{log}_{10} 2^N}
,
\end{equation}
revealing that compared to the direct quantization of the input signal the signal-to-noise ratio is increased by a factor of
\begin{equation}
\mathrm{G}_p = \frac{\EX(\tilde{\xi}(n)^2)}{\EX(\delta(n)^2)} 
\end{equation}
termed as the \textbf{prediction gain}, being a large number, assuming that the input signal can be predicted precisely.
This arises the question, how the actual input sample can be estimated based on the previous samples only.

\subsection{The optimal prediction coefficients}
As the most simple approach the actual input sample $\xi(n)$ can be predicted as the linear combination of the previous $N$ number of samples, written in the form of
\begin{equation}
\tilde{\xi}(n) = \sum_{m=1}^N p(m) \xi(n-m) = \mathbf{p}^{\mathrm{T}} \mathbf{\xi}_{n-1},
\label{Eq:lin_pred}
\end{equation}
written in a vectorial form.
In the expression vector $\mathbf{p} = [p(1), p(2),...,p(N)]^{\mathrm{T}}$ contains the weights of the previous input samples used for prediction, and vector $\mathbf{\xi}_{n-1} = [\xi(n-1),\xi(n-2),...,\xi(n-N)]^{\mathrm{T}}$ contains the previous $N$ number of the input samples.

The goal is to minimize the expected energy of the difference between the actual input sample $\xi(n)$ and the prediction $\hat{\xi}(n)$ by optimizing the prediction weights $\mathbf{p}^{\mathrm{T}}$ so that 
\begin{equation}
\arg\min\limits_{\mathbf{p}}: \EX\left( | \xi(n) - \tilde{\xi}(n) |^2 \right) = \arg\min\limits_{\mathbf{p}}: \EX\left( | \xi(n) - \mathbf{p}^{\mathrm{T}} \mathbf{\xi}_{n-1} |^2 \right) 
\end{equation}
holds.
The quadratic expression can be expounded to
\begin{multline}
\EX\left( | \xi(n)^2 - \mathbf{p}^{\mathrm{T}} \mathbf{\xi}_{n-1} |^2 \right) =
\EX\left( \xi(n) - 2\xi(n)\mathbf{p}^{\mathrm{T}} \mathbf{\xi}_{n-1}  + \mathbf{p}^{\mathrm{T}} \mathbf{\xi}_{n-1}  \mathbf{\xi}_{n-1}^{\mathrm{T}} \mathbf{p}\right) = \\
\EX\left( \xi(n)^2 \right) - 2 \mathbf{p}^{\mathrm{T}} \EX\left(  \xi(n) \mathbf{\xi}_{n-1} \right) + \mathbf{p}^{\mathrm{T}} \EX\left(  \mathbf{\xi}_{n-1}  \mathbf{\xi}_{n-1}^{\mathrm{T}} \right) \mathbf{p}
\end{multline}
with exploiting the linearity of expected value operator and collecting non-stochastic quantities outside of it.
The expected value of the scalar-vector product and the dyadic product terms of the expression can be recognized as the autocorrelation values of the input signals, rewritten in a matrix form as
\begin{equation}
\EX\left( | \xi(n)^2 - \mathbf{p}^{\mathrm{T}} \mathbf{\xi}_{n-1} |^2 \right) =
r_{\xi}(0) - 2 \mathbf{p}^{\mathrm{T}} \mathbf{r}_{\xi} + \mathbf{p}^{\mathrm{T}} \mathbf{R}_\xi \mathbf{p}
\label{Eq:quad_eq}
\end{equation}
with denoting the signal energy, and the autocorrelation vector and matrix as
\begin{align}
\begin{split}
r_{\xi}(0) = \EX\left( \xi(n)^2 \right), \hspace{15mm}
\mathbf{r}_{\xi} =  \begin{bmatrix}
       r_\xi(1) \\[0.3em]
       r_\xi(2) \\[0.3em]
       ... \\[0.3em]
       r_\xi(N) \end{bmatrix}, \\
\mathbf{R}_{\xi} =  \begin{bmatrix}
       r_\xi(0) & r_\xi(1) & ... & r_\xi(N-1) \\[0.3em]
       r_\xi(1) & r_\xi(0) & .... & r_\xi(N-2)\\[0.3em]
       ... \\[0.3em]
       r_\xi(N-1) & r_\xi(N-2) & ... & r_\xi(0)\end{bmatrix}.
\end{split}
\end{align}
Expression \ref{Eq:quad_eq} has to be minimized with respect to vector $\mathbf{p}^{\mathrm{T}}$.
The minimization can be performed by finding the zero of the derivative of the expression with respect to vector $\mathbf{p}^{\mathrm{T}}$, reading 
\begin{equation}
\frac{\partial}{\partial \mathbf{p}^{\mathrm{T}}}\EX\left( | \xi(n)^2 - \mathbf{p}^{\mathrm{T}} \mathbf{\xi}_{n-1} |^2 \right) =
- 2  \mathbf{r}_{\xi}  + 2\mathbf{R}_\xi \mathbf{p} = 0.
\end{equation}
Finally, from the above equation the optimal prediction coefficient vector can be expressed as
\begin{equation}
\mathbf{p}^{\mathrm{T}} =
\mathbf{R}_\xi^{-1} \mathbf{r}_{\xi}.
\label{Eq:optimal_coefs}
\end{equation}
The above coefficients are the so-called \textbf{Wiener filter} coefficients for the estimation of a stationary stochastic process.

From the form of the optimal prediction coefficients it is clear that the signal estimation is based on the measured correlation of the previous source samples, therefore prediction is efficient as long as neighboring samples are linearly related.
Hence the optimal prediction is often termed as \textbf{linear prediction}.
The above Wiener filter is, therefore, capable of the estimation of the correlated part of the input signal.

\subsection{Prediction as FIR filtering}


It should be noted that linear prediction \eqref{Eq:lin_pred} describes the discrete linear convolution of vectors $\mathbf{p}$ and $\mathbf{\xi}_{n-1}$. 
This means that the estimation of the actual sample can be obtained by the simple FIR filtering\footnote{The term FIR (Finite Impulse Response) filtering refers to the fact that the applied filter contains no feedback, thus it is ensured that to an excitation with finite extent the filter output is of finite extent.
The actual filter impulse response is described by the coefficient vector itself.} of the input stream with the coefficient vector $\mathbf{p}$.
The result of estimation is subtracted from the input sample, generating the differential signal, which, therefore, can be written as
\begin{equation}
\delta(n) = \xi(n) - \sum_{m=1}^{N} p(m) \xi(n-m),
\end{equation}
or, transforming the equation to the $z$-transform domain---by exploiting that delay by one sample is a multiplication by $z^{-1}$ in the $z$-domain---as
\begin{equation}
\delta(z) = \xi(z)\left( 1  - \sum_{m=1}^{N} p_m(z) \, z^{-m}\right) = \xi(z)\left( 1 - P(z) \right).
\label{Eq:lin_pred_filter}
\end{equation}
The realization of linear prediction with FIR is depicted in Figure \ref{Fig:FIR_prediction}.
\begin{figure}[]
	\centering
	\begin{overpic}[width = 1\columnwidth ]{figures_en/lin_pred_fir.png}
	\small
	\end{overpic}
	\caption{One dimensional and two dimensional white noise process (a) and correlated noise process (b).}
	\label{Fig:FIR_prediction}
\end{figure}
The structure of FIR filtering is depicted in Figure \ref{Fig:FIR_prediction} (b).
The prediction filter can be also interpreted as an accumulator, or memory, containing the previous samples, added with different weights to the output.

\vspace{3mm}
It is important to note that the prediction filter $P(z)$ is capable of identifying linear tendencies in the input signal and the actual sample is estimated based on the assumed linear relationship between previous samples
Once all the correlated part of the input signal is removed, by definition, in the remaining differential signal each sample is uncorrelated from the previous samples.
Thus, with theoretical optimal prediction, filter $\left( 1  -  P(z) \right)$ \textbf{decorrelates} the input signal, and the differential signal is a \textbf{white noise process}:
The filter is often referred to as whitening filter, since in the optimal case it transforms the input signal into white noise. 


Figure \ref{Fig:whitening} illustrates the whitening process in case of a simple 1D input signal, in the present example being an audio stream.
The correlation of the signal can be estimated based on the input stream according to \eqref{Eq:correlation_Def} from which the linear prediction coefficients are obtained (from \eqref{Eq:optimal_coefs}).
These coefficients are applied to perform whitening filtering process described by \ref{Eq:lin_pred_filter}.

Figure \ref{Fig:whitening} (a) compares the time histories of the \href{https://github.com/gfirtha/MMTA_lecture_notes/blob/master/Samples/prediction_in.wav}{input} and the \href{https://github.com/gfirtha/MMTA_lecture_notes/blob/master/Samples/prediction_out.wav}{differential} signals.
\begin{figure}[]
	\centering
	\begin{overpic}[width = 1\columnwidth  ]{figures_en/linear_prediction.png}
	\small
	\put(25,30){(a)}
	\put(0,0){(b)}
	\put(51,0){(c)}
	\end{overpic}
	\caption{Example for the optimal linear prediction of an audio input stream.}
	\label{Fig:whitening}
\end{figure}
As it is illustrated the dynamics range of the input signal is significantly reduced by subtracting the predicted signal.
Generally speaking, periodic signals can be predicted efficiently by measuring correlation, therefore, harmonic signals are almost entirely removed from the input.
Obviously, transients can not be predicted based on previous samples, therefore, they are still present in the differential signals.

Figure \ref{Fig:whitening} (b) verifies that as the effect of filtering the autocorrelation of the differential signal became approximately a delta function, hence the output stream is nearly uncorrelated.
This is also verified by comparing the input and output spectral density functions, with the output spectrum being nearly constant as depicted in Figure \ref{Fig:whitening} (c).

Note that since the input signal is typically of low-pass filtered characteristics, therefore, the whitening filter has to be of high-pass characteristics in order to flatten out the output spectrum.
This statement can be generalized to typical audio and video signals: since natural signals are usually dominated by low-frequency content, therefore, in practical applications the differential signal (i.e. prediction) is obtained as the high-pass filtered version of the input signal.

\vspace{3mm}
It has been highlighted that once the autocorrelation of the input signal can be estimated (e.g. by measurement) nearly optimal prediction coefficients can be defined.
Hence, the prediction filter $P(z)$ depends on the actual input signal.
Obviously, the decoding of the differential signal also requires the knowledge of coefficients $P(z)$ in the decoder side.
In order to avoid the transmission of the prediction filter coefficients, in practice as a sub-optimal solution fixed prediction coefficients are applied for high-pass filtering.
In the following only this fix-coefficient approach is considered.

\subsection{Problem of feedforward prediction}

Although being a very simple approach, the direct FIR filtering scheme, presented in Figure \ref{Fig:FIR_prediction} (a) is never used directly in practice, due to the following reason:

So far only the prediction and generation of the differential signal have been discussed in details, without taking the receiver side into consideration.
In the receiver---with assuming that the prediction filter coefficients are known---the original samples are reconstructed by adding the previously accumulated, weighted samples to the residual signal.
With denoting the decoded signal by $\tilde{\xi}$ it can be written as
\begin{equation}
\tilde{\xi}(z) = \delta'(z) + \sum_{m=1}^{N} p_m(z) \tilde{\xi}(z) z^{-m} \hspace{3mm} \rightarrow \hspace{3mm}
\frac{\tilde{\xi}(z)}{\tilde{\delta}(z)} = \frac{1}{1-P(z)},
\end{equation}
hence, in the receiver the original signal can be reconstructed by \textbf{inverse filtering} and the transfer function of the inverse filter is the reciprocal of the forward filter.
Obviously, $\frac{1}{1-P(z)}$ describes an IIR (Infinite Impulse Response) filter, since the reconstruction in the receiver is performed by feedbacking the output signal to the input of the receiver.
Hence, the zeros of the forward FIR filter are mapped to the poles of the inverse filter, leading to stability problems in the receiver.

\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.8\columnwidth ]{figures_en/feedforward_enc_dec.png}
	\small
	\put(0,0){(a)}
	\put(5,24){$\xi(z)$}
	\put(25,14){$\hat{\xi}(z)$}
	\put(26,24){$\delta(z)$}
	\put(45,24){$\delta'(z)$}
	\put(50,0){(b)}
	\put(94,24){$\tilde{\xi}(z)$}
	\end{overpic}
	\caption{Example for the optimal linear prediction of an audio input stream.}
	\label{Fig:feedforward_enc_dec}
\end{figure}

These stability problems causes serious artifacts due to the presence of the quantizer:
As discussed earlier the presence of quantizer can be modeled as introducing quantization noise, added directly to the differential signal.
Hence, with denoting the $z$-transform of the additive quantization noise by $\epsilon(z)$ and taking both the encoding and decoding steps into consideration the decoded signal can be written as
\begin{equation}
\tilde{\xi}(z) = \left( \underbrace{\xi(z) \left( 1 - P(z) \right)}_{\delta(z)} + \epsilon(z) \right) \cdot \frac{1}{1 - P(z) }
=
\xi(z) + \epsilon(z) \frac{1}{1-P(z)}.
\label{eq:reproduced_signal}
\end{equation}
The decoded signal, therefore, consists of the original input signal and the quantization noise, filtered with the potentially unstable inverse filter.

In order to highlight the resulting effect as a simple example DPCM encoding is investigated.

\paragraph*{DPCM coding with feedforward prediction:\\}
As the simplest approach for differential coding the basis of prediction is simply the previous sample of the input signal, hence 
\begin{align}
\begin{split}
\delta(n) &= \xi(n)-\xi(n-1) \\ P(z) &= z^{-1} \hspace{3mm} \rightarrow \hspace{3mm} 1 - P(z) = 1 - z^{-1}.
\end{split}
\label{Eq:temp1}
\end{align}
The differential sample is simply given as the difference of the actual and the previous input samples.
The approach is termed as \textbf{Differential Pulse-Code Modulation}.

Generally speaking, the derivative of the a continuous function most simply can be approximated numerically by the finite difference
\begin{equation}
\frac{\partial}{\partial t}f(t) \approx \frac{f(t)-f(t-T)}{T} = \frac{1}{T} \left( f(n) - f(n-1) \right),
\end{equation}
termed as the backward Euler scheme, with $T$ being the sampling interval.
Comparison of this expression with \eqref{Eq:temp1} clearly reveals that differential coding realizes the approximate differentiation of the input signal prior to quantization.
The frequency response of this simple differentiation is obtained analytically by evaluation the $z$-transform along the unit circle, i.e. by the substitution $z = \mathrm{e}^{\mathrm{j} 2\pi f/f_s}$, with $f_s$ being the sampling frequency
\begin{equation}
|1-z^{-1}| = |1 - \mathrm{e}^{-\mathrm{j} 2\pi f/f_s}| = 2 \left|\sin \frac{\pi f}{f_s} \right|.
\end{equation}
The frequency response of the filter is shown in Figure \ref{Fig:differentiator_integrator} (a) along with the frequency response of an ideal differentiator.
The filter has a zero at $z=1$, at zero frequency (as the derivative of a constant signal is zero), and approximates an ideal differentiator perfectly in the low frequency region.
\begin{figure}[]
	\centering
	\begin{overpic}[width = 1\columnwidth  ]{figures_en/differentiator_integrator.png}
	\small
	\put(0,0){(a)}
	\put(50,0){(b)}
	\end{overpic}
	\caption{Example for the optimal linear prediction of an audio input stream.}
	\label{Fig:differentiator_integrator}
\end{figure}

Obviously, the inverse operation of differentiation is integration, i.e. the inverse filter $\frac{1}{1-P(z)}$ describes the numerical approximation of integration.
This is verified by Figure \ref{Fig:differentiator_integrator} (b), depicting the frequency response of the inverse filter and that of an ideal integrator, with a pole at zero frequency (the integral of constant signal tends to infinity).

As a conclusion, the drawback of the simple feedforward processing scheme---where the encoder performs prediction directly from the input signal---is the following:
According to \eqref{eq:reproduced_signal} the quantization noise is reproduced in the output of the decoder filtered with the inverse filter $\frac{1}{1-z^{-1}}$.
Since this filter response describes the integration of the filtered signal, therefore, the quantization noise is integrated, \textbf{accumulated} in the decoder.
Less formally speaking: the source of accumulation of quantization error is the fact that due to the presence of quantization the basis of prediction is different in the encoder and the decoder side.
While the encoder predicts the next sample from the original signal, in the decoder side only the quantized, decoded previous samples are available for the next prediction.

\subsection{Feedback prediction loop}

In order to overcome the error of feedforward prediction and to avoid the accumulation of the quantization error it has to be ensured that the encoder and the decoder uses the same past samples for prediction.
This can be achieved only by ensuring that the encoder predicts from the quantized input samples as well.
Hence, in the encoder side the quantized differential signal has to be decoded, which decoded signal will serve as the basis for the next prediction: the decoder has to be built in the encoder in a feedback loop.

\begin{figure}[]
	\centering
	\begin{overpic}[width = 1\columnwidth ]{figures_en/fb_diff_quant.png}
	\small
	\put(1,38){$\xi$}
	\put(36,38){$\delta'$}
	\put(35,20){$\delta'$}
	\put(35,10){$\xi'$}
	\put(29,18){$\hat{\xi}$}
	\put(13.5,31.5){$\hat{\xi}$}
	\put(0,0){(a)}
	\put(58,0){(b)}
	\end{overpic}
	\caption{One dimensional and two dimensional white noise process (a) and correlated noise process (b).}
	\label{Fig:feedback_diff}
\end{figure}

The concept of differential quantizer with built-in decoder stage is depicted in Figure \ref{Fig:feedback_diff} (a):
the accumulator/filter $P(z)$ contains the previous quantized samples (in the previous example only the previous, quantized sample) and produces the actual estimation $\hat{\xi}'$ as their weighted sum.
In the actual time step the estimation is subtracted from the actual input signal, and the difference is quantized, producing the quantized differential output signal.
Besides transmission to the receiver, the quantized differential signal is added to the estimated sample, producing the decoded, quantized signal $\xi'$, which is pushed into the accumulator and will serve as the basis of the prediction in the next time step.
Hence, it is ensured that the basis of prediction is the quantized signal, similarly to the receiver side.

Mathematically the entire built in decoder can be modeled as a single transfer function:
The input of the decoder block is the quantized differential signal $\delta'$, while the output is the estimation $\hat{\xi}$, i.e.
\begin{equation}
R(z) = \frac{\hat{\xi}(z)}{\delta'(z)} = \frac{P(z) \left( \hat{\xi}'(z) + \delta'(z) \right) }{\delta'(z)} \hspace{0.3cm}
\rightarrow \hspace{0.3cm}
R(z) = \frac{P(z)}{1-P(z)}
\end{equation}
holds.
The transmitted differential signal can be written with taking the quantization error into consideration as
\begin{align}
\begin{split}
\delta'(z) &= \xi(z) - R(z)\delta'(z) + \epsilon(z) \hspace{3mm} \rightarrow \hspace{3mm} \left( 1 + R(z) \right) \delta'(z) = \xi(z) + \epsilon(z) \\
\delta'(z) &=  \frac{1}{1-R(z)} \left( \xi(z) + \epsilon(z) \right) = (1-P(z)) \left( \xi(z) + \epsilon(z) \right)
\end{split}
\end{align}
and finally after decoding by applying the unchanged decoder with the transfer function of $\frac{1}{1-P(z)}$ the decoded signal reads
\begin{equation}
\tilde{\xi}(z) = \frac{1}{1-P(z)}(1-P(z)) \left( \xi(z) + \epsilon(z) \right) = \xi(z) + \epsilon(z).
\end{equation}
Therefore, without the accumulation or the error, besides the original input the decoded signal contains only the actual quantization noise.
Due to this reason in differential coding exclusively the above feedback structure is applied with the encoder containing the built-in decoder stage.

\subsection{Application of predictive coding in video technologies}

Predictive coding is extensively applied in the field of image and video compression.
Prediction can be applied either between pixels or blocks of the image, performing spatial prediction, or between consequent images, applying temporal prediction:
\begin{itemize}
\item \textbf{Temporal prediction:} MPEG video encoders apply block based temporal DPCM prediction, meaning that each block of a frame is predicted from a block from the previous frame.
Hence, the prediction filter/accumulator contains a single block of the previous image, which is subtracted from the actual input block.
Newer compression methods, like H.264, allows prediction using multiple reference blocks with arbitrary weights, therefore, the length of the prediction filter may be larger than one, approximating the optimal prediction case, discussed in the foregoing.
The block in the prediction filter, termed as the \textbf{reference block}, does not necessarily located in the same position as the actual input block, but its location is found where the the reference block resembles the most to the actual input block.
Finding the position of the reference block is the basic task of \textbf{motion estimation.}
Motion estimation and temporal prediction of the MPEG encoder is discussed in details in the following chapter.
%
\item \textbf{Spatial prediction:} Alternatively, within an image each pixel may be predicted based on the neighboring pixel values, followed by requantization of the difference pixel.
Assuming that pixel values change slowly over the image, storing only the difference from the previous pixels may result in significant data compression.
Pixel based predictive coding is applied by the PNG image encoder as discussed in the following, while modern video compression methods, e.g. H.264 allows more sophisticated block based intra-image prediction.

Furthermore, all JPEG and MPEG block-based video encoders stores the average luminance/color of the actual block predictivly compared to the previous block in a DPCM loop.

As a rule of thumb, since in the decoder side only the requantized difference pixels/blocks are available, in order to avoid the accumulation quantization error predictive coding is in all cases performed within a feedback prediction loop.
\end{itemize}

\paragraph*{PNG encoding:\\}

\begin{figure}[h!]
	\centering
	\begin{overpic}[width = 0.8\columnwidth ]{figures_en/png_1.png}
	\small
	\put(0,0){(a)}
	\end{overpic}
	\begin{overpic}[width = 0.8\columnwidth ]{figures_en/png_2.png}
	\small
	\put(0,0){(b)}
	\end{overpic}
	\caption{Prediction schemes (a) and block diagram (b) of PNG encoder.}
	\label{Fig:png_encoder}
\end{figure}
As a simple example for spatial prediction the PNG encoder is discussed briefly.
PNG (Portable Network Graphics), being one of the simplest image compression methods apply a lossless spatial prediction along with lossless entropy coding, with the following properties:
\begin{itemize}
\item Since PNG is a lossless compression, therefore, no quantization is present in the encoder.
\item As PNG is lossless, luma/chroma separation  is not beneficiary, but direct RGB coordinates are encoded.
\item Compression is achieved by the fact that the result of spatial prediction contains mainly small pixel intensities, clustered around 0, which can be efficiently entropy coded with a properly chosen variable-length coding.
The entropy coder of PNG is termed as \href{https://en.wikipedia.org/wiki/DEFLATE}{DEFLATE} coding, being originally introduced for the early version of ZIP comression.
\item Before DEFLATE compression the actual pixel, denoted in Figure \ref{Fig:png_encoder} by $x_{m,n}$ is predicted from the neighboring pixels.
Obiviously, only previously encoded and transmitted pixels can be the reference of prediction, denoted by $z_{m,n}$.
The number of reference pixels can be chosen by the encoder from the prediction schemes, shown in Figure \ref{Fig:png_encoder}.
Therefore, in this case linear prediction is based on a 2D FIR filter, containing 2, 3 or 4 values in the prediction accumulator, denoted by $h_{k,l}$.
\item Note that in this exclusive case, since no quantization is performed, feedback prediction may be interchangeable with feedforward structure.
\end{itemize} 

%TODO : Here should go and example of predicted image. E.g. Lenna: https://www.eecs.wsu.edu/~cs445/Lecture_19.pdf
%http://www.cmlab.csie.ntu.edu.tw/cml/dsp/training/coding/

\section{Transform coding}
Alternatively to linear prediction, potential linear dependence between samples can be eliminated by representing the input data in a new representation space as the linear combination of properly chosen basis vectors (or matrices).
The change of basis can be performed by applying a linear transform to the input.
With an optimal choice of the linear transform the weights of the basis vectors/matrices---i.e. the representation of the data in the new basis---are less correlated, with reduced spatial redundancy, and allows more efficient quantization adapted to the properties of human vision.

\begin{figure}[h!]
	\centering
	\begin{overpic}[width = 0.8\columnwidth ]{figures_en/transform_coding.png}
	\end{overpic}
	\caption{General block scheme of image transform encoder.}
	\label{Fig:transform_coding}
\end{figure}
The general processing scheme for image coding is depicted in Figure \ref{Fig:transform_coding}.
In image processing applications usually block-based coding is applied, with the input image segmented to $N\times N$ sized pixel blocks.
The input blocks are transformed by a properly chosen 2D linear transform, representing the pixel block as the weighted sum of basis images.
Similarly to prediction, the linear transform itself is theoretically reversible.
The irreversible, lossy coding is usually the requantization of the transformed data, followed by an efficient reversible entropy coding method.

\subsection{1D and 2D linear transforms}

Before discussing the 2D linear transforms applied frequently in image coding, the general theoretical basics of linear transforms are revisited.

\paragraph*{One-Dimensional Transforms:\\}
Let $x(n) = [x(0),\, x(1),\,...,\, x(N-1)]^{\mathrm{T}}$ denote the \textbf{input vector} $\mathbf{x}$ of $N$ samples and $y(k) = [y(0),\, y(1),\,...,\, y(N-1)]^{\mathrm{T}}$ the \textbf{transform vector} $\mathbf{y}$ with the same size.

The connection of the input and transform vectors is established by 
\begin{equation}
y(k) = \sum_{n=0}^{N-1} x(n) \, A(k,n), \hspace{5mm} \text{for} \, k = 0,1,...,N-1,
\end{equation}
or in matrix notation
\begin{equation}
\mathbf{y} = \mathbf{A} \mathbf{x},
\end{equation}
where $A(k,n)$ is the forward transformation kernel, forming the $\mathbf{A}$ \textbf{forward transform matrix}, and $y(k)$ are termed as the transform coefficients. 
The inverse transform that recovers the input sequence is given by
\begin{equation}
x(n) = \sum_{k=0}^{N-1} y(k) \, B(k,n), \hspace{5mm} \text{for} \, n = 0,1,...,N-1,
\label{Eq:inv_tr}
\end{equation}
or in matrix notation
\begin{equation}
\mathbf{x} = \mathbf{B} \mathbf{y},
\end{equation}
where $\mathbf{B}$ is the \textbf{inverse transform matrix}, satisfying $\mathbf{B} = \mathbf{A}^{-1}$.

Let $\mathbf{b}_k$ denote the $k$-th basis vector of the new representation space.
The inverse transform \eqref{Eq:inv_tr} can be rewritten with a slightly modified notation
\begin{equation}
x(n) = \sum_{k=0}^{N-1} y(k) \, b_k (n), \hspace{5mm} \text{for} \, n = 0,1,...,N-1,
\end{equation}
clearly reflecting the fact that the inverse transform reproduces the original input vector as the weighted sum of the basis vectors, and the weights are given by the transform coefficients $y(k)$.
Hence, linear inverse transform can be interpreted as the series expansion of the input signal.
The series expansion can be written in a matrix form as
\begin{equation}
\begin{bmatrix}[c]
      \\
   \mathbf{y}  \\
     \\
\end{bmatrix}
= 
\underbrace{\begin{bmatrix}[c|c|c|c]
      &   &  & \\
   \mathbf{b}_0 & \mathbf{b}_1 & ... & \mathbf{b}_{N-1} \\
     &   &   & \\
\end{bmatrix}}_{\mathbf{B}} \cdot 
\begin{bmatrix}[c]
      \\
   \mathbf{x}  \\
     \\
\end{bmatrix}.
\end{equation}
The above expression shows that generally speaking, the columns of the inverse transform matrix are the basis vectors of the new representations space.

\vspace{3mm}
In the aspect of image compression the class of \textbf{orthogonal transform}---or in case of complex valued basis vectors the \textbf{unitary transforms}---are of special importance.
In this case the basis vectors form an orthonormal set, their scalar product satisfying 
\begin{equation}
\mathbf{b}_i^{*\mathrm{T}} \cdot \mathbf{b}_j = \sum_{n = 0}^{N-1} b_i^*(n) \cdot b_j(n) = \delta_{ij} = \begin{cases} 1 \hspace{3mm} \text{if} \, i = j
\\ 0 \hspace{3mm} \text{otherwise},
\end{cases}
\end{equation}
with $*$ denoting complex conjugate.
Therefore, in case of a real orthogonal transform the rows and the columns of the inverse transform matrix are orthonormal, and
\begin{equation}
\mathbf{B}^{*\mathrm{T}} \mathbf{B} = \mathbf{E} \hspace{3mm} \rightarrow \hspace{3mm}
\mathbf{B}^{*\mathrm{T}}  = \mathbf{B}^{-1} = \mathbf{A}
\end{equation}
holds, where $\mathbf{E}$ is the identity matrix.

Therefore, in case of a real, orthogonal transform the rows of the forward transform matrix contain the basis vectors:
\begin{equation}
\begin{bmatrix}[c]
\\     
   \mathbf{x}  \\
\\     \\
\end{bmatrix}
= 
\underbrace{\begin{bmatrix}[c c c c c]
     & &   \mathbf{b}_0^{\mathrm{T}} & & \\ 
     & &  \mathbf{b}_1^{\mathrm{T}}  & &  \\
     & &  ...  &  & \\ 
     & &  \mathbf{b}_{N-1}^{\mathrm{T}} & & \\
\end{bmatrix}}_{\mathbf{A}} \cdot 
\begin{bmatrix}[c]
    \\   
   \mathbf{y}  \\
   \\    \\
\end{bmatrix},
\end{equation}
and the $k$-th element of the coefficient vector is calculated as
\begin{equation}
y(k) = \sum_{k=0}^{N-1} x(n) \, b_k (n), \hspace{5mm} \text{for} \, k = 0,1,...,N-1,
\end{equation}
describing the scalar product of the $k$-th basis vector and the input vector.
Hence, in case of a orthogonal (or unitary) transform the transform coefficients are obtained by projecting the input vector to the basis vectors.

Orthogonality is clearly a necessary property for basis vectors that are used to decompose an input into uncorrelated components in an $N$-dimensional space, otherwise linear relationship exist between the basis vectors.
Orthonormality of basis vector is a stronger property ensuring the preservation of signal energy in the transform space following
\begin{equation}
\left| \mathbf{y} \right|^2 = \mathbf{y}^{\mathrm{T}} \mathbf{y} = \left(\mathbf{A}\mathbf{x}\right)^{\mathrm{T}} \mathbf{\mathbf{A} \mathbf{x}} = \mathbf{x}^{\mathrm{T}} \underbrace{ \mathbf{A}^{\mathrm{T}}\mathbf{A} }_{\mathbf{E}} \mathbf{x} = \mathbf{x}^{\mathrm{T}} \mathbf{x}  =| \mathbf{x} |^2.
\end{equation}
This is the Parseval-theorem.
Transform coding applies the quantization of the transform coefficients, introducing error in the transform space.
Orthogonality ensures that the average energy of error in the inverse transformed signal equals the error variance introduced in the quantization of transform coefficients.


\paragraph*{Two-Dimensional Transforms:\\}
One-dimensional transforms can be easily generalized towards the two-dimensional case.
The forward and inverse transforms generalized as
\begin{align}
\begin{split}
Y(k,l) &= \sum_{n=0}^{N-1} \sum_{m=0}^{N-1} X(n,m) \, A(k,l,m,n), \hspace{5mm} \text{for} \, k,l = 0,1,...,N-1,
\\
X(m,n) &= \sum_{k=0}^{N-1} \sum_{l=0}^{N-1} Y(k,l) \, B(k,l,m,n), \hspace{5mm} \text{for} \, m,n = 0,1,...,N-1,
\end{split}
\label{Eq:2D_Tr}
\end{align}
with $X(m,n)$ being the $N\times N$ sized input matrix, representing e.g. non-overlapping pixel intensities of an input image, $Y(k,l)$ the coefficient matrix, and $A(k,l,m,n)$ and $B(k,l,m,n)$ being the forward and inverse transform hyper-matrices.
By using notation $A(k,l,m,n) \rightarrow A_{k,l}(m,n)$ the orthogonal 2D transform and inverse transform is defined as
\begin{align}
\begin{split}
Y(k,l) &= \sum_{n=0}^{N-1} \sum_{m=0}^{N-1} X(n,m) \, A_{k,l}(m,n), 
\\
X(m,n) &= \sum_{k=0}^{N-1} \sum_{l=0}^{N-1} Y(k,l) \, A^{\mathrm{T}}_{k,l}(m,n).
\end{split}
\end{align}
Clearly, $A_{k,l}^{\mathrm{T}}(m,n)$ describes the $k,l$-th \textbf{basis matrix}, with the forward transform representing the projection of the input block to the actual basis matrix, while the inverse transform describes the expansion of the input matrix as the weighted sum of basis matrices.
The coefficient matrix $Y(k,l)$ gives the weight of the $k,l$-th basis matrix in the input image.

\vspace{3mm}
The evaluation of the 2D transforms of \eqref{Eq:2D_Tr} may be computationally expensive.
In practice \textbf{separable transforms are applied} in which case the transform hyper-matrices can be written as a product of two matrices
\begin{equation}
A(k,l,m,n) = A_1(k,n) \cdot A_2(l,m),
\end{equation}
meaning that the basis matrices $ A_{k,l}(m,n)$ can be also written
\begin{equation}
A_{k,l}(m,n) = A_{k}(m) \cdot A_{l}(n)
\end{equation}
as the product of a row and column vector (i.e. as a dyadic product).

In this case the forward transform takes the form 
\begin{align}
\begin{split}
Y(k,l) &= \sum_{n=0}^{N-1} \sum_{m=0}^{N-1} X(n,m) \,  A_1(k,n) \cdot A_2(l,m) = \\
&= \sum_{m=0}^{N-1} A_2(l,m) \cdot \left( \sum_{n=0}^{N-1} A_1(k,n)  X(n,m) \right),
\end{split}
\end{align}
or written in a matrix form
\begin{equation}
\mathbf{Y} = \left( \mathbf{A}_2 \left( \mathbf{A}_1 \mathbf{X} \right)^{\mathrm{T}} \right)^{\mathrm{T}}= 
\left( \mathbf{A}_2 \mathbf{X}^{\mathrm{T}} \mathbf{A}_1^{\mathrm{T}} \right)^{\mathrm{T}} = 
\mathbf{A}_1 \mathbf{X} \mathbf{A}_2^{\mathrm{T}} .
\end{equation}
Obviously, left-multiplication by $\mathbf{A}_1$ describes the 1D linear transform of each column of $\mathbf{X}$ by the transform described by $\mathbf{A}_1$, while right-multiplication describes the 1D linear transform of each rows of the matrix, described by $\mathbf{A}_2$.
Hence, a separable transform can be written as two consequent vertical and horizontal 1D transforms, allowing efficient numerical evaluation.
Similarly, the orthogonal inverse transform can be written as
\begin{equation}
\mathbf{X} = \left( \mathbf{A}_2^{\mathrm{T}} \left( \mathbf{A}_1^{\mathrm{T}} \mathbf{Y} \right)^{\mathrm{T}} \right)^{\mathrm{T}}= 
\left( \mathbf{A}_2^{\mathrm{T}} \mathbf{Y}^{\mathrm{T}} \mathbf{A}_1 \right)^{\mathrm{T}} = 
\mathbf{A}_1^{\mathrm{T}} \mathbf{Y} \mathbf{A}_2.
\end{equation}
In most cases the vertical and horizontal transforms are identical ($\mathbf{A}_1 = \mathbf{A}_2 = \mathbf{A}$).
In this case the 2D transform can be evaluated by 
\begin{equation}
\mathbf{Y} = \mathbf{A} \mathbf{X} \mathbf{A}^{\mathrm{T}}, \hspace{1cm} 
\mathbf{X} = \mathbf{A}^{\mathrm{T}} \mathbf{Y} \mathbf{A}.
\end{equation}

In image processing the basis matrix $A_{k,l}^{\mathrm{T}}(m,n)$ is usually referred to as \textbf{basis images}, referring to the fact that inverse transform describes the original image block as the weighted sum of the basis images.
Similarly, forward transform is the decomposition of the input image to basis images.

\subsection{Transforms coding of images}

The basic goal of representing input image blocks in the transform space is to find a representation which can be quantized more efficiently than the direct quantization of the image.
The efficiency of a linear transform can be qualified as follows:
\begin{itemize}
\item The linear transform is usually followed by requantization the transformed coefficients.
If the transformed representation is \textbf{compact}---meaning most of the input energy is distributed amongst a few coefficients---then basis vectors with low energy can be either quantized more coarsely, or can be completely omitted in the inverse transform by applying a truncated series expression.
This can lead to significant compression.
Due to the Parseval theorem, the energy of the quantization error is, of course, the same in the spatial domain, but quantization noise will be distributed among the pixels uniformly.

Hence, the efficiency of the transform applied is described by the compactness of the representation \footnote{As a simple example the Fourier transform of a constant signal contains only a DC coefficient, hence, the spectral representation is much more compact than the spatial one.}.
\item Alternatively, the same requirement can be formulated via correlation in the transform domain:
In order to reduce spatial redundancy, i.e. correlation of the pixel values, the transformed coefficient should be uncorrelated.
It can be verified that the compactness of representation is completely equivalent with being uncorrelated (so the above two requirements are equivalent).
\end{itemize}

Defining a measure for the compactness of the transform is not straightforward.
On the other hand, the correlation of the transformed coefficients can be easily defined, allowing the derivation of an optimal linear transform: the Karhunen-Loeve transform

\subsection{The Karhunen-Loeve transform}

Being the optimal solution the Karhunen-Loeve transform (KLT) is defined so that in the transform space the coefficients are completely correlated.
The goal is then, with a known input signal the linear transform has to be defined, which decorrelates the input samples.
Note that the basic problem is very similar to the optimal linear prediction approach, which results in filter coefficients, completely estimating the correlated content of an input signal, resulting in a completely decorrelated differential signal.

In order to arrive at the KLT solution assume that the input vector is a realization of a wide-sense stationary stochastic process, denoted by $x(n)$.
Let $\mathbf{R}_x$ denote the correlation matrix of the input signal, with the elements given as
\begin{equation}
R_x(m,n) = \EX( x(m) \cdot x(n) )
\hspace{3mm} \rightarrow \hspace{3mm}
\mathbf{R}_x = 
  \begin{bmatrix}
       r_x(0) & r_x(1) & ... & r_x(N-1) \\[0.3em]
       r_x(1) & r_x(0) & .... & r_x(N-2)\\[0.3em]
       ... \\[0.3em]
       r_x(N-1) & r_x(N-2) & ... & r_x(0)\end{bmatrix}
\end{equation}
giving the estimated correlation between the elements of the input registry (similar to the concept used in optimal prediction theory).
The correlation matrix is therefore a symmetric (in case of complex input a hermitian) matrix.
Furthermore, assume that the eigendecomposition of the correlation matrix is known\footnote{For a symmetric matrix the eigendecomposition is guaranteed to exist with the eigenvectors forming a full orthonormal basis} so that it can be written in the form
\begin{equation}
\mathbf{R}_x  = \mathbf{U}_x  \mathbf{D}_x  \mathbf{U}_x ^{\mathrm{T}}, \hspace{1mm} \text{with} \hspace{1mm}
\mathbf{D}_x  =   
\begin{bmatrix}
       \lambda_0 & 0 & ... & 0 \\[0.3em]
       0 & \lambda_1 & .... & 0\\[0.3em]
       ... \\[0.3em]
       0 & 0 & ... & \lambda_{N-1}\end{bmatrix},
       \hspace{2mm}
\mathbf{U}_x  =       \begin{bmatrix}[c|c|c|c]
      &   &  & \\
   \mathbf{v}_0 & \mathbf{v}_1 & ... & \mathbf{v}_{N-1} \\
     &   &   & \\
\end{bmatrix}.
\end{equation}
In the decomposition the diagonal matrix $\mathbf{D}_x $ contains the eigenvalues of $\mathbf{R}_x$ in its diagonal, and matrix $\mathbf{U}_x $ contains the eigenvectors of $\mathbf{R}_x$ in its columns.

\vspace{3mm}
By definition, a process is uncorrelated only if its correlation matrix is diagonal (all the samples are correlated only with themselves).
Hence, the optimal transform is found for a given input vector, so that the correlation matrix of the result of transformed vector is diagonal, being the basic idea behind KLT.
The correlation matrix of the transformed vector $\mathbf{y}$ is obtained as
\begin{align}
\begin{split}
R_y(m,n) &= \EX( y(m) \cdot y(n) ) = \\
&\EX(  \sum_{m=0}^{N-1} A(k,m) \, x(m) \cdot \sum_{n=0}^{N-1} A(l,n) \, x(n)  ) = \\ 
&\sum_{m=0}^{N-1} A(k,m) \sum_{n=0}^{N-1} A(l,n) \, \EX(  x(m) \cdot  x(n)  ) = \\
&\sum_{m=0}^{N-1} A(k,m) \sum_{n=0}^{N-1} A(l,n) R_x(m,n),
\end{split}
\label{eq:eig}
\end{align}
which can be written in a matrix form as
\begin{equation}
\mathbf{R}_y  = \mathbf{A}^{\mathrm{T}} \mathbf{R}_x  \mathbf{A}
\end{equation}
As and educated guess the transform matrix is chosen as $\mathbf{A} = \mathbf{U}_x$, and substituting the eigendecomposition \eqref{eq:eig} into $\mathbf{R}_x$ the correlation matrix reads as
\begin{equation}
\mathbf{R}_y  = \underbrace{\mathbf{U}_x^{\mathrm{T}} \mathbf{U}_x  }_{\mathbf{E}}  \mathbf{D}_x  \underbrace{\mathbf{U}_x ^{\mathrm{T}} \mathbf{U}_x }_{\mathbf{E}}  = \mathbf{D}_x 
\end{equation}
The autocorrelation of the transform vector is, thus, a diagonal matrix.

The optimal solution, therefore, constructs the transform matrix as its columns being the eigenvectors of the input signal's autocorrelation matrix.
As a result in the transform domain the coefficients are completely decorrelated.
This statement is equivalent to that the KLT transform of the input signal gives the most compact representation in a sense that it reduces the total mean-square error resulting of the truncation of the inverse transform series expansion.

\vspace{3mm}
The Karhunen-Love transform can be easily extended towards 2D input matrices.
In this case the a 4D hyper-matrix gives the correlation between the samples of the 2D input matrix.
The optimal transform hyper-matrix is then obtained from the eigenmatrices of the correlation hyper-matrix in the same manner.

\vspace{3mm}
However, similarly to optimal prediction, the KLT transform depends on the actual input registry, and in order to decode (inverse transform) the original image the transform matrix has to be transmitted along with the transformed signal.
Therefore, in image compression KLT is rarely used, instead it is frequently applied in image analysis, e.g. in machine image recognition.
Instead, in image compression sub-optimal, fixed-basis transforms are used.
The most frequently used sub-optimal linear transforms are the non-harmonic Welsh-Hadamard, Hartley, Haar, Slant transforms and the harmonic Discrete Fourier, Sine and Discrete Cosine transforms (DCT).

\subsection{Discrete Cosine Transform (DCT)}

\subsubsection*{1D DCT}

\subsubsection*{2D DCT}