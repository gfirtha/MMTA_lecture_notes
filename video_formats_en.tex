The previous chapter introduced the representation of color pixels in analog and digital systems.
The current chapter presents how analog and digital video signals can be formed by using these pixel representations and discusses how the parameters of the resulting video formats were chosen.

\section{Structure and properties of the video signal}

The discussion starts with the questions that arose at the establishment of the early analog TV broadcast systems, beginning with the format parameters elaborated for the NTSC and PAL standards and later adopted by the SD digital format.
These analog systems are, of course, already superseded by digital transmission and broadcasting standards.
However, their discussion is still of great importance, on one hand because the principles of their actual parameter choices hold unchanged for introducing modern systems as well.
On the other hand, a large number of parameters, used in HD and UHD formats are the legacy of these early systems.

\subsection{Structure of the analog video signal}

Before discussing the actual video parameters the structure of the video signals is introduced.
The term video signal refers to the signal, carrying video information either from a broadcasting video source, or between local electronic devices (e.g. from set-top-box to the TV on a HDMI interface, or from a computer/laptop to an external display through VGA interface).
Even today, the structure of the digital video signal is completely identical with the analog representation, that's structure is the result of the operation principle of early CRT displays.

In the previous chapter \ref{sec:CRT} gave a detailed explanation on the principle of cathode-ray tube based imaging devices:
The realization of the RGB primaries was achieved by phosphores covering the screen, emitting visible light when excited.
The excitation was ensured by an electron gun, producing a narrow electron beam, with the current density being approximately proportional to the 2.2 power of the input voltage (hence the original need for gamma correction).

The electron beams (one in case of black and white, three beams in case of colored display), driven with the input voltage of the video signal, are focused and steered by properly driven magnetic coils, and scan the display screen line by line, along \textbf{scan lines} in a predefined \textbf{raster scanning pattern}\footnote{The choice of scanning the screen linewise is not absolutely obvious, while creating the early black and white TV standards other solution, e.g. columnwise or back-and-fortha linewise scanning types were also investigated.}.
As the electron beam reaches the end of a given scan line, it retraces horizontally to the beginning of the next line.
Similarly, by reaching the lower end of the screen the beam retraces vertically to the beginning of the next frame.

An important principle of CRT displays is that the video data is received and displayed real-time, instantaneously (when early TV receivers were introduced circuits existed for storing video data).
Hence, the analog video signal is basically the driving voltage of the CRT electron guns, containing the video data line by line: 
In case of a black and white display, the video signal consists of the continuous luma information $Y'$, while in color receivers the receiver either receives directly the $R'G'B'$ signals (not used in video broadcasting), or demodulates them from the luma-chroma representation.
The principle of raster scanning is depicted in Figure \ref{Fig:TV_signal} (a).
%
\begin{figure}[]
	\centering
	\begin{minipage}[c]{0.3\textwidth}
		\begin{overpic}[width = 1\columnwidth ]{figures/FormatJargon-1.png}	
		\small
		\put(0,0){(a)}		
		\end{overpic}
				\begin{overpic}[width = 1\columnwidth ]{figures/TV.png}	
		\small
		\put(0,0){(d)}		
		\end{overpic}
	\end{minipage} \hfill
	\begin{minipage}[c]{0.68\textwidth}
		\centering
		\begin{overpic}[width = 0.86\columnwidth ]{figures/FormatJargon-3.png}		\small
		\put(0,0){(b)}		\end{overpic}
		\begin{overpic}[width = 1\columnwidth ]{figures/FormatJargon-7.png}		\small
		\put(0,0){(c)}		\end{overpic}
	\end{minipage}
%
	\caption{General structure of analog video signal}
	\label{Fig:TV_signal}
\end{figure}
%

Obviously, it takes a finite time for the electron beam to retrace from the end of a line to the beginning of the next one, and similarly to retrace from the end of the screen to the beginning.
During these \textbf{retracing intervals} the electron beam is turned off, i.e. \textbf{blanked}, otherwise undesired traces would appear on the screen.
In practice this means that during these \textbf{blanking intervals} the video signal is zero, or negative valued, i.e. it takes black level, or synch level.
In the blanking interval synchronizing pulses are added to the video signal that ensure that the oscillators in the receiver remain locked with the transmitted signal, so that the image can be reconstructed on the receiver screen.

Based on the foregoing the linewise video signal contains three distinct time intervals:
\begin{itemize}
\item The \textbf{active video content}, containing the actual video information of one line.
In analog formats the length of the active video interval is expressed in $\mu \mathrm{s}$, and in $\mathrm{px}$-s in the digital case.
\item The \textbf{horizontal blanking interval}, the time it takes for the electron beam to retrace from the end of the actual scan line to the beginning of the next scan line.
The horizontal blanking interval contains the horizontal sync pulse (\textbf{HSYNC}) with negative signal level (i.e. ,,blacker than black''), serving as the trigger pulse for the horizontal retracing circuit.
The sync pulse is separated from the active video data with black level intervals before and after the sync pulse, termed as the front porch and black porch.
The length of horizontal blanking is expressed in $\mu \mathrm{s}$ in the analog, and in $\mathrm{px}$-s in the digital case.
\item The \textbf{vertical blanking interval (VBI)}, the time it takes for the electron beam to retrace from the end of the actual frame to the beginning of the next frame.
The vertical blanking interval contains the vertical sync pulses (\textbf{VSYNC}) at negative signal level, allowing the vertical synchronization of the display frames on the screen.
The length of the VBI is expressed in lines (number of the inactive lines).
\end{itemize}
The video signal, therefore, contains in one entire line period time ($T_{\mathrm{H}}$, for horizontal) both \textbf{active video interval} and \textbf{inactive interval}, carrying no video information.
The structure of video lines is depicted in Figure \ref{Fig:TV_signal} (b).
Similarly, the entire period time of one frame ($T_{\mathrm{V}}$, as vertical) contains \textbf{active lines} and \textbf{inactive lines}, containing no actual video data.
The structure of active and inactive lines is depicted in Figure \ref{Fig:TV_signal} (c).

In the following it is revealed what principles lead to the actual timing parameters (e.g. the lengths of the above intervals, line frequency, frame frequency) and to the standard definition resolution parameters. 

\subsection{Parameters of analog video formats}

\subsubsection*{Aspect ratio and display size}

First it is explained what display size should be the optimal format parameters chosen for.

It was already discussed in \ref{Sev:HVS} that the human color vision is ensured by the macula, containing mostly cones, located around the center of the retina.
In the center of the macula the fovea is responsible for sharp central vision (also called foveal vision).
Due to the size of the fovea, the human central vision with high visual acuity covers about $10-15^{\circ}$ from the entire field of view of $\approx 200^{\circ}$ horizontally.
During the creation of standard definition television standard the basic goal was to fill only the central vision with content, therefore, the SD television should cover about $10^{\circ}$ from the horizontal field of view (i.e. the peripheral vision does not contribute to imaging).
Obviously, the actual display size then depends on the viewing distance, as it will be discussed later in this chapter.

\vspace{3mm}
Another important spatial attribute of video formats is the ratio of the horizontal and vertical screen size, i.e. the \textbf{aspect ratio} ($a_r$).
The basis of all the color TV formats was the NTSC standard and its black and white predecessor, introduced in the 1940s.
As an obvious endeavor the introduced broadcasting format was engineered in order to be compatible with the then-existing video sources, e.g. movie films.
Until the 1950s during the entire silent film era movies were almost exclusively captured with the aspect ratio of 4:3, i.e. the ratio of the horizontal and vertical screen size was $1.3\dot{3}$
\footnote{
The introduction of the aspect ratio of 4:3 is connected to the work of Thomas Alva Edison, who \href{https://en.wikipedia.org/wiki/35_mm_movie_film}{defined} the standard image exposure length on $35~\mathrm{mm}$ film for movies is four perforations ($19~\mathrm{mm}$) per frame along both edges. 
From the available film width between perforations ($25.375~\mathrm{mm}$) the active are has an aspect ratio of 4:3.
This \href{https://en.wikipedia.org/wiki/Negative_pulldown}{4-perf negative pulldown} became the official standard in 1909, allowing the emerging of standard movie cameras, movie projectors, and hence emerging of cinema technologies.}.
Although by the 1950s first widescreen movie formats have already emerged, the NTSC standard adopted the \textbf{4:3} aspect ratio, which remained the standard TV and video aspect ratio until the introduction of the HD format.

%TODO anamorphic lenses?
% Forrás: https://www.shutterstock.com/blog/4-3-aspect-ratio
% https://www.cinematographers.nl/FORMATS1.html

\subsubsection*{Refresh rate and frame rate}

The next parameter to investigate is the temporal sampling frequency of the video data, i.e. the number of frame fed to the display device per second.
First three closely related terms are introduced:
\begin{itemize}
\item The \textbf{refresh rate} ($\mathbf{f_{\mathrm{r}}}$) refers to the number of times in a second that a display ,,flashes'', i.e. redraws its content, expressed usually in $\mathrm{Hz}$.
\item The \textbf{frame rate} ($\mathbf{f_{\mathrm{V}}}$) express the number of time in a second that the content of the display changes, i.e. the number of frames, contained by the video signal per second, usually expressed in $\mathrm{fps}$ (frame per second) or in $\mathrm{Hz}$.
\item The \textbf{field rate} ($\mathbf{f_{\frac{\mathrm{V}}{2}}}$) can be defined for interlaced video data, denoting the number of fields (half frames) per second in the video data, generally expressed in $\mathrm{fps}$.
Universally $f_{\frac{\mathrm{V}}{2}} = 2\cdot f_{\mathrm{V}}$ holds.
\end{itemize}
The key difference between refresh rate and frame rate is that refresh rate is the property of the display device (e.g. LCD display) and includes the repeated drawing of identical frames, while frame rate measures how often a video source can feed an entire frame of new data to a display.

In order to arrive at practical frame rate and refresh rate choice for video data two perceptual aspects have to be taken into consideration:
\begin{itemize}
\item On one hand when reproducing objects under motion it is crucial to display sufficient number of motion phases in order to ensure that the observer perceives a smooth, continuous motion.
This requirement constitutes a lower limit for the applicable frame rate.
\item On the other hand the refresh rate has to be chosen high enough in order to avoid \textbf{flickering} and to reduce eye strain.
\end{itemize}
Due to the \textbf{beta movement} phenomenon the frame rate can be significantly lower, than the refresh rate:
Beta movement is an optical illusion whereby viewing a rapidly changing series of static images creates the illusion of a smoothly flowing scene, occurring if the frame rate is greater than 10 to 12 $\mathrm{fps}$. 
Therefore, due to the beta movement the frame rate should satisfy
\begin{equation}
f_{\mathrm{frame}} > \sim20~\mathrm{Hz},
\end{equation}
\footnote{
It is worth noting that the above frame rate only allows the perception of motion instead of distinct static images, higher frame rates )(usually $60~\mathrm{fps}$) still ensure much smoother motion reproduction.
In order to increase the effective frame rate of the video stream, modern displays and software allow temporal interpolation by estimating the intermediate frames, based on some motion prediction algorithm, similarly to MPEG encoding.
However, the average viewer already adapted to the frame rate of $24~\mathrm{fps}$, being the standard frame rate of movie films, therefore the increased frame rate often generates an unpleasant, unnatural effect.
This is termed as the \textbf{soap opera effect}, originating from the fact that usually low cost TV shows are recorded directly to digital video cameras (being much cheaper than capturing to film), allowing higher frame rates, usually set to $60 \mathrm{fps}$.}
however, applying the same refresh rate would lead to serious perceived flickering artifacts.

In over to avoid flickering the refresh rate has to be higher than the \textbf{flicker fusion threshold}
\footnote{
The flickering fusion threshold is the frequency at which an intermittent light stimulus appears to be completely steady to the average human observer.
For surfaces with temporally alternating luminances above the flickering threshold the average luminance is perceived.
The threshold depends on numerous factors:
Depends on the average illumination intensity, the adaptation state, the color of vibration (above $15-20~\mathrm{Hz}$ fluctuation of hue information can not be perceived) of and the position on the retina at which the stimulation occurs}.
For the purposes of presenting moving images, the human flicker fusion threshold is usually taken between 60 and $90~\mathrm{Hz}$:
in the central vision, dominated by the cones the response time is high, and the flickering fusion threshold is around $50~\mathrm{Hz}$.
The peripheral vision is dominated by the rods, with a much lower response time and the flickering fusion threshold is higher.

In case of analog TV formats the goal was to fill the central vision of the viewer with, therefore, with a refresh rate of
\begin{equation}
f_{\mathrm{r}} > 50-60~\mathrm{Hz} 
\end{equation}
flickering can be avoided\footnote{This is true only for the central vision.
The flickering of CRTs can be easily observed with a display watched from the peripheral vision}.
The choice of the actual refresh rate was, however, a consequence of the CRT technology's imperfection: a trick in order to avoid the effect of the supply voltage ripple.

\begin{figure}[]
	\centering
	\begin{overpic}[width = 1\columnwidth ]{figures/ripple.png}
	\end{overpic}
	\caption{Source of the supply voltage ripple in a single-way rectifier}
	\label{Fig:ripple}
\end{figure}

\paragraph*{Effect of the mains frequency:}
Voltage ripple is the residual periodic variation of the DC voltage in a power supply due to the imperfect rectification of the alternating means AC voltage, as illustrated in \ref{Fig:ripple}.
The ripple frequency coincides with the means frequency in case of single-way rectification, or with its double in the two-way case.
As in a CRT display the anode is directly connected to the supply voltage, therefore, any perturbation in the DC voltage is directly added to the video signal and is displayed on the screen.

Assume a screen consisting of $N_{\mathrm{V}}$ horizontal scan lines, with the refresh rate denoted by $f_\mathrm{r}$.
The number of scan lines displayed in a second, i.e. the line frequency is given by
\begin{equation}
f_\mathrm{H} = N_\mathrm{V} \cdot f_\mathrm{r}.
\end{equation}
In order to investigate the effect of supply ripple, as a generalization, assume a periodic black and white video signal, oscillating between 0 and 1, described by
\begin{equation}
Y(t) = \frac{1}{2} \left( 1 + \sin 2 \pi f t \right),
\end{equation}
with $f$ being the signal frequency. 
For the sake of simplicity the blanking intervals are assumed to be of zero length.
In this case the sine wave is displayed on the screen line by line.
Depending on the signal frequency $f$ the content of the screen can be the following:
\begin{itemize}
\item If $f = f_\mathrm{H}$ each scan line consists of exactly one period of the sine wave, resulting in a horizontal sine on the screen, as depicted in Figure \ref{Fig:ripple_display} (a).
\item If $f > f_\mathrm{H}$ each scan line consists of less then one period of the sine wave, with the initial phase in the beginning of the scan lines increasing line by line and frame by frame.
Therefore, the horizontal sine on the screen is slightly steered, and moves towards left.
Similarly, with $f < f_\mathrm{H}$ the image moves slowly towards right.
\item If $f = f_\mathrm{r}$ each frame consists of a single period of the sine wave.
Assuming $N_\mathrm{V} \gg 1$ the value of the sine function changes insignificantly over one scan line, therefore, the displayed image is a vertical sine.
\item If $f > f_\mathrm{r}$ the initial phase of the vertical sine increases frame by frame, thus, the sine wave moves slowly upwards.
Similarly, for $f < f_\mathrm{r}$ the vertical sine moves downwards.
%
\end{itemize}
\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.45\columnwidth ]{figures/horizontal_sine.png}
	\small
	\put(0,0){(a)}
	\end{overpic}
	\hspace{5mm}
	\begin{overpic}[width = 0.45\columnwidth ]{figures/vertical_sine.png}
	\small
	\put(0,0){(b)}
	\end{overpic}
	\caption{Periodic video signal with $f = f_\mathrm{H}$ (a) and $f = f_V$ (b) as displayed line by line}
	\label{Fig:ripple_display}
\end{figure}
%
Based on the foregoing, with the appropriate synchronization of the signal frequency and the refresh rate a periodic video signal can be displayed as a still, non-moving image.
The effect of voltage ripple acts as such a periodic noise signal, with the frequency determined by the mains voltage of the given region and displayed inevitably on the screen.
Early tests with black and white TV receiver suggested the visible effect of this periodic noise of less disturbing if the noise image is still.
Therefore, both in the American, and later in the European systems refresh rate was chosen to coincide the mains frequency
\footnote{
In the American NTSC system with the introduction of color information the choice of refresh rate became more complicated, as the frequency of the color subcarrier could not be correctly chosen.
Without more details: as a consequence both the refresh rate (and the field rate) and the line frequency had to be decreased by $0.1~\%$.
Therefore, in the American system the refresh rate is $f_r = 60\cdot \frac{1000}{1001} = 59.94~\mathrm{Hz}$.
Due to the presence of vertical and horizontal sync pulses, this change did not had and effect on the then-existing TV receivers.}.
\begin{equation}
f_{\mathrm{r,USA}} = 60~\mathrm{Hz}, \hspace{1cm} f_{\mathrm{r,Eu}} = 50~\mathrm{Hz}
\end{equation}
ensuring that supply ripple only manifests in a non-moving vertical noise image on the screen.

Having found the refresh rate for early TV systems, which has been also in use by modern CRTs and LCD displays, still, the actual frame rate is an open question.
An obvious choice for the frame rate would be the refresh rate itself.
Due to bandwidth efficiency, however, instead a special raster scan order was introduced. 

\subsubsection*{Raster scan orders}

The raster scan order is the rectangular pattern of image capture and reconstruction in television.
In the receiver side it defines the systematic process how the electron beam scans the entire screen over a given time interval.
The following section introduces two frequently used raster scan order, leading to the definition of frame rate and field rate for.

Obviously, for currently used LCD and OLED displays the raster scan order can not be interpreted as the scanning pattern of the display, since the entire display content is updated in the same time instant.
Still, scan order can be understood also as the storing and transmission order of video data, hence scan order is interpretable in modern video systems as well.

\paragraph{Progressive scan:}
As the most straightforward scanning order, \textbf{progressive scan} is a format of displaying, storing, or transmitting moving images in which all the lines of each frame are drawn in sequence.
In the receiver side it means that the electron beam scans and updates the content of the entire screen over one frame time ($T_{\mathrm{V}} = 1/f_{\mathrm{V}}$), line by line.
\begin{figure}[]
	\centering
	\begin{minipage}[c]{0.6\textwidth}
	\begin{overpic}[width = 1\columnwidth ]{Figures/progressive_scan.png}
	\end{overpic}   \end{minipage}\hfill
		\begin{minipage}[c]{0.3\textwidth}
	\caption{Illustration of progressive scanning (for the sake of simplicity with the exemplary number of 11 lines), with the active line content (1), the vertical retrace/blanking interval (2) and the horizontal retrace/blanking interval (3).}
	\label{Fig:progressive}  \end{minipage}
\end{figure}
Progressive scanning is illustrated in Figure \ref{Fig:progressive}.

In the aspect of transmission in case of video data with progressive scanning the content of the entire frames has to be transmitted over the frame time via the given interface (e.g. HDMI) sequentially.
Progressive scan is usually denoted by letter ,,p'' in the format designation.

Although progressive scan seems to be the most simple and obvious scan order, still, until the emergence of the UHD format progressive scan was only occasionally utilized due to the reasons discussed in the following.

\vspace{3mm}
\paragraph{Interlaced scan:}
The previous section have already discussed that in order to avoid flickering the display refresh rate should be risen above $50~\mathrm{Hz}$, while for ensuring continuous motion the video frame rate of around $20~\mathrm{Hz}$ may be sufficient.
This fact suggests that frame rate should be handled independently by choosing a high refresh rate and a efficiently low frame rate, allowing the reduction of video data and the required bandwith for transmission.

This concept could be easily implemented in cinematic techniques:
As a heritage of the early silent movie era (where the frame rate varied between $16-24~\mathrm{Hz}$) movie films are commonly captured with the frame rate of 24 frames per second.
In order to avoid flickering the film projectors are equipped with special \href{https://www.youtube.com/watch?v=jrSzRAch930}{two or three blade shutters}, rotating in front of the projector beam and flashing each frame two or three times before the film would travel to the next frame.
With the simple trick of presenting the same frame multiple times the effective refresh rate can be increased to $48\mathrm{fps}$ or $72~\mathrm{fps}$, and flickering may be avoided.

Also, nowadays frame rate and refresh rate can be handled independently in digital systems (e.g. in a video card and a VGA monitor), where a \textbf{display buffer} is present, containing the data of an entire frame, generating and feeding the video signals towards the display.
Modern LCD displays are usually built with the LED backlight panel flashing at around $200~\mathrm{Hz}$\footnote{
Obviously, the LED panel could be built with continuous driving voltage of well, instead of flashing, but the dynamic adjustment of its lightness (based on the video content) can be much cost-efficiently solved with PWM modulation.}.
Still, as a rule of thumb even in the presence of a video buffer the content of the video buffer may change during the vertical blanking of the display in order to avoid \href{https://en.wikipedia.org/wiki/Screen_tearing#/media/File:Tearing_(simulated).jpg}{screen tearing}.
As a consequence, generally $f_{\mathrm{r}} = \mathrm{n} \cdot f_{\mathrm{V}}$ holds, where $\mathrm{n} \in \lbrace 1,2,3,... \rbrace$.

In analog TV receiver no frame buffer could be present due to the lack of appropriate technology, the transmitted signal had to be displayed by the receiver on the fly.
Obviously, transmitting the same frames multiple times requires a significant increase of bandwidth. 
Therefore, a more simple engineering workaround was needed for the bandwidth efficient video transmission, which was eventually provided by the concept of \textbf{interlace scanning}.

\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.85 \columnwidth ]{Figures/interlaced_scan.png}
	\end{overpic}
	\caption{Illustration of interlaced scanning (for the sake of simplicity with the exemplary number of 21 lines)
	Assuming that scanning starts with the beginning of the first scan line, in order to cover the entire screen the first field has to end in a half-line, and the second field has to start with a half-line.
	This requirement can be satisfied only by applying odd number of scan lines (each field consists of $N_{\frac{V}{2}} + \frac{1}{2}$ lines, thus the total line number is given by $2 N_{\frac{V}{2}} + 1$, being an odd number by definition). }
	\label{Fig:interlaced}
\end{figure}

The basic concept of interlaced scanning is illustrated in Figure \ref{Fig:interlaced}:
Instead of scanning each line of the display within one frame period time, the screen is divided into two \textbf{fields}
\begin{itemize}
\item the \textbf{odd field}, containing every odd scan line 
\item the \textbf{even field}, containing every even scan numbered scan line
\end{itemize}
The interlaced signal contains the two fields of a video frame captured consecutively (i.e. the even and odd fields are captured in consequent time instants).
Similarly, in the receiver the electron beam scans first all the odd lines, displaying the content of the odd field, then draws the content of the even field into all the even lines.
Interlaced scan is usually denoted by letter ,,i'' in the format designation.

By applying interlaced scanning the content of the display is redrawn with the frequency of the fields, thus, for interlaced video the refresh rate is given by the \textbf{field rate} ($f_{\frac{\mathrm{V}}{2}}$).
Therefore, the field rate is determined by the mains frequency of the given region, being $50~\mathrm{Hz}$ for the European and $60~\mathrm{Hz}$ (more precisely $59.94~\mathrm{Hz}$) for the American electric network.
Furthermore, the period time of an entire frame, consisting both even and odd fields is obviously twice the field period time, thus the frame rate is half of the field rate.
As a result in the frame rate is $25~\mathrm{Hz}$ in the European and $30~\mathrm{Hz}$ ($29.97~\mathrm{Hz}$) in the American systems, being high enough to ensure the perception of continuous motion, while the refresh rate is sufficient in order to avoid flickering.
As a summary for the foregoing in the European and American system 
\begin{align}
\begin{split}
f_{\mathrm{r,USA}} &= f_{\frac{\mathrm{V}}{2},\mathrm{USA}} = 2\cdot f_{\mathrm{V},\mathrm{USA}} = 60~\mathrm{Hz} , \hspace{1cm}  f_{\mathrm{V},\mathrm{USA}}= 30~\mathrm{Hz} \\
f_{\mathrm{r,Eu}} &= f_{\frac{\mathrm{V}}{2},\mathrm{Eu}} =  2\cdot f_{\mathrm{V},\mathrm{Eu}} = 50~\mathrm{Hz}, \hspace{1cm}  f_{\mathrm{V},\mathrm{Eu}}= 25~\mathrm{Hz}
\end{split}
\end{align}
hold.

Opposed to the solution of cinema, i.e. displaying the same frame multiple times the even and odd fields in interlaced video are taken in consequent time instants.
Therefore, in interlaced video the temporal resolution is effectively doubled at the cost of halving the vertical spatial resolution of the video data.
This means the following properties of interlaced video
\begin{itemize}
\item Compared to progressive video with the same refresh rate, interlaced scanning achieves the compression factor of 2:1, resulting in halved analog bandwidth
\item In case of still, and slowly moving scenes the vertical resolution coincides with the progressive resolution (since the even and odd fields complete the same frame)
\item In case of rapid motion the vertical resolution if half of the progressive resolution
\end{itemize}
Generally speaking for video content with slowly changing content, e.g. movies, interlaced scanning ensures a sufficiently high vertical resolution with improved motion reproduction besides feasible signal bandwidth.
In case of fast motion, e.g. camera movements in sport programs the artifacts due to the reduced vertical resolution become visible.
\vspace{3mm}

As bandwidth efficiency was of primary importance during the introduction of early analog television broadcasting, all the analog and digital standard definition video formats adopted exclusively interlaced scanning.
Later the high definition video standard introduced progressive video formats for the first time, while the latest ultra high definition format supports progressive scanning mode only.

\vspace{3mm}
\paragraph{Questions of interlaced scanning:}
Besides its obvious advantages, the application of interlaced video rises a number of questions and challenges.

As an example, it introduces the phenomenon of \textbf{interline twitter}:
The previous chapter explained that the violation of the sampling theorem---i.e. sampling frequency components above half the sampling frequency---leads to spatial aliasing phenomena.
In the field of image processing it manifests in visible Moiré pattern on spatially periodic textures (e.g. a brick wall, or squared shirts).
In case of interlaced video the vertical resolution is halved, compared to progressing scanning, therefore, vertical aliasing artifacts are more likely to emerge.
Since the even and odd fields are displayed alternatively, therefore also the potential Moiré patterns alternate from field to field.
This may produce a shimmering effect, termed as twittering, even when a still image is \href{https://en.wikipedia.org/wiki/File:Indian_Head_interlace.gif}{reproduced}.
Interline twittering is the main reason, why television professionals avoid wearing clothing with fine striped patterns, while professional video cameras apply a low-pass filter to the vertical resolution of the signal to prevent interline twitter. 

\begin{figure}  
\small
  \begin{minipage}[c]{0.64\textwidth}
	\begin{overpic}[width = 1\columnwidth ]{Figures/Interlaced_video_frame_(car_wheel).jpg}
	\end{overpic}   \end{minipage}\hfill
	\begin{minipage}[c]{0.3\textwidth}
    \caption{
    Displaying interlaced video on a progressive display without deinterlacing.}
\label{fig:deinterlacing}  \end{minipage}
\end{figure}

Further questions arise in case of the conversion between interlaced and progressive formats.
The progressive to interlaced conversion can be straightforwardly solved by dividing the entire frame to odd and even lines.
Interlaced to progressive conversion emerges more frequently in practice: e.g. in case of the playback of a DVD disc on a usually progressive computer monitor.
As the simplest solution, two adjacent field can be combined to form a single frame, however, this naive approach leads to the ,,combing'' of moving objects on the screen, as illustrated in Figure \ref{fig:deinterlacing}.
Generally speaking techniques for the interpolation of the content of intermediate scan lines are termed as \textbf{deinterlacing} methods.
Deinterlacing is an often emerging problem even today, as broadcasting most often transmits interlaced HD format (generally using format 1080i), while modern LCD displays do not support native interlaced scanning anymore.


\section{Analog video formats}

\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.8\columnwidth ]{figures_en/NTSC_PAL.png}
	\end{overpic}
	\caption{Distribution of analog television formats by nation}
	\label{Fig:NTSC_map}
\end{figure}

Based on the foregoing, the main properties of the video format used for analog broadcasting can be introduced.
Historically, three analog video formats were introduced with the advent of color television broadcasting:
\begin{itemize}
\item The first color TV broadcast system was the \textbf{NTSC} format, introduced by the Federal Communications Commission (FCC) in the United States in 1954 and have been adopted by western America and Japan.
For vertical resolution NTSC selected $N_V = 525$ scan lines\footnote{
The choice of the vertical line number was a compromise between the existing black and white systems, e.g. as used by RCA's NBC TV network, and the intention of manufactures, desiring to increase the line number to 600-800.
The actual value originates from a limitation of the CRT technology of the day:
In early TV receivers a master oscillator ran at twice the line-frequency, and this frequency ($2\cdot 15750~\mathrm{Hz}$) was divided down by the number of lines used (in this case $N_V = 525$) to give the field frequency ($f_{\frac{\mathrm{V}}{2}} = 60~\mathrm{Hz}$).
This frequency was compared with the mains frequency ($60~\mathrm{Hz}$ in America) and the master oscillator's frequency was corrected by the discrepancy in order to avoid a moving noise image due to supply ripple.
At the time, frequency division was performed use of a chain of multivibrators, the overall division ratio being the mathematical product of the division ratios of the chain.
Since the line number has to be odd an odd number (see previous section), therefore it can be factorized only to odd numbers as well, which had do be relatively small in order to avoid thermal drift of the oscillator.
The closest sequence up to about 500 lines that meets all these requirements is $525 = 3\cdot 5\cdot 5 \cdot 7$, giving the number of scan lines.}
with interlaced scanning and the	 field rate of $f_{\mathrm{\frac{V}{2}}} = 60~\mathrm{Hz}$ in agreement with the mains frequency.
\item As an improved version of NTSC, the \textbf{PAL} system was introduced by the European Broadcasting Union (EBU) in 1967 and besides Europe it was adopted in Australia, South America, Africa and a part of Asia.
The PAL format applied interlaced scanning with the number of scan lines of $N_V = 625$ along with the field rate of $50~\mathrm{Hz}$.
\item Also as an improvement on NTSC, emerging from France the \textbf{SECAM} system was introduced in France and in the Soviet Union.
The SECAM system will not be discussed in details in the present book.
\end{itemize}
The main parameters of the NTSC and PAL formats are summarized in Table \ref{tab:sd_formats}.
As a summary it can be stated that NTSC provides a higher temporal resolution (i.e. higher refresh rate/field rate) with lower spatial resolution, while PAL ensures a higher spatial resolution due to the increased number of scan lines at the cost of lower field rate.

\begin{table}[h!]
\caption{Parameters of the NTSC and PAL analog formats}
\renewcommand*{\arraystretch}{2.25}
\label{tab:sd_formats}
\begin{center}
    \begin{tabular}[h!]{ @{}c | | l | l @{} }%\toprule
				         &   NTSC  							       & PAL \\ \hline
    Total line number ($N_{\mathrm{V}}$):	 &  525   								   &  625 \\
    Number of active lines ($N_{\mathrm{V,A}}$):   &  480   								   &  576 \\
    Frame rate ($f_{\mathrm{V}}$):    &  $30~\mathrm{Hz}$ ($29.97~\mathrm{Hz}$) & $25~\mathrm{Hz}$ \\
    Field rate ($f_{\frac{\mathrm{V}}{2}}$): &  $60~\mathrm{Hz}$ ($59.94~\mathrm{Hz}$) & $30~\mathrm{Hz}$ \\
    Line frequency ($f_{\mathrm{H}}$):    &  $525 \cdot 30 = 15750~\mathrm{Hz}$ ($15734~\mathrm{Hz}$) & $15625~\mathrm{Hz}$ \\
    Line period time ($T_{\mathrm{H}}$):           &  $63.49~\mathrm{\mu s}$ ($63.55~\mathrm{\mu s}$) & $64~\mathrm{\mu s}$ \\
    Active line time ($T_{\mathrm{H,a}}$):           &  $\approx 52 ~\mathrm{\mu s}$ ($63.55~\mathrm{\mu s}$) & $52~\mathrm{\mu s}$ \\
    \end{tabular}
\end{center}
\end{table}

\begin{figure}[t!]
\captionsetup{singlelinecheck=off}
\small
  \begin{minipage}[c]{0.64\textwidth}
	\begin{overpic}[width = 1\columnwidth ]{Figures/Timing_PAL_FrameSignal.png}
	\end{overpic}
    \end{minipage} \hfill
	  \begin{minipage}[c]{0.3\textwidth}
    \caption[]{ The line structure of an entire frame in case of interlaced scan, illustrated for a single video component:
    \begin{itemize}
    \item active line interval: grey
    \item horizontal blanking interval: magenta, cyan and yellow
	\item vertical blanking interval: green, orange and white
    \end{itemize}
    }\label{fig:PAL_frame}
    \end{minipage}
\end{figure}
The structure of an entire frame (i.e. two consecutive fields) in the PAL system is illustrated in Figure \ref{fig:PAL_frame}.
Obviously, the figure depicts only a single video component, the content of the components is discussed in the following.
The figure depicts the horizontal and vertical blanking intervals, with the VSYNC (denoted by green color) and the HSYNC (yellow interval) sync pulses.
These pulses ensure the synchronization of the display to the received signal by triggering the vertical and horizontal retrace of the electron beam, therefore, it is ensured that the individual ,,pixel'' values are displayed on the correct position.
With the loss of these sync pulses the displayed image is misaligned vertically (in case of lost VSYNC), resulting in the \textbf{jitter} or horizontally (in case of lost HSYNC) causing the \textbf{rolling} of the video signal.

Although the characteristics of the video signal---with containing vertical and horizontal blanking intervals---originates from the operation principle of CRT displays, even in modern digital systems (e.g. in case of HDMI or SDI interfaces) the structure of the video signal is identical with the presented analog one.
Obviously, modern LCD displays do not need the presence of blanking intervals at all, since they update the content of all the pixels quasi-instantaneously, still, the blanking intervals are present in digital video signals as well.
One reason for this is that digital video signal is the legacy of the CRT era (and even today, professional CRT studio monitors are often employed), with the newly introduced standards all based on the previous ones.
On the other hand the blanking intervals allow the transmission of auxiliary data, including teletext, captions, subtitles, and in case of digital standards, \textbf{audio streams} of the media content as well.
The actual location of the digital data is codified in ITU-R BT.1364 and SMPTE 291M.
As and example, the audio streams accompanying video data are positioned in the vertical blanking intervals in the HDMI standards, i.e. between the content of active lines
\footnote{
As a simple example for audio transmission via HDMI interface:
In case of a HD format of 1080p (with the total number of lines 1125), with the frame rate of $60~\mathrm{Hz}$, the line frequency is given by $f_V = 67.5~\mathrm{kHz}$.
Assuming an audio stream with 8 channels, sampled at $f_s = 192~\mathrm{kHz}$ the data to be transmitted over one entire frame is $\frac{8 \cdot 192 000 }{60} = 25600~\mathrm{samples}$.
This means the transmission of 23 samples transmitted with one video line, which results in the maximum bandwidth of the HDMI 1.0 \href{https://www.sciencedirect.com/science/article/pii/B9780128016305000049}{standard}.}.

\vspace{3mm}
% https://www.sciencedirect.com/topics/computer-science/blanking-interval
% https://www.sciencedirect.com/topics/computer-science/horizontal-blanking
So far only the general structure of the video signals was discussed, without the investigation of the actual data in the active intervals, e.g. in case of analog video, without investigating what the actual video signals are.
Obviously, the representation of one color pixel requires the transmission of three individual video components, which in the field of video transmission is most often the luma-chroma representation.

Based on the number of actual individual transmitted components video signals can be categorized to two main formats 
\begin{itemize}
\item \textbf{Component video} transmits the video signal on three individual signal paths, i.e. on three individual cable pairs.
\item \textbf{Composite video} transmits the three video signals multiplexed into one single signal, transmitted over a single path, or physically speaking on one pair of cables.
\end{itemize}
\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.90\columnwidth ]{figures/video_comp2.png}
	\end{overpic}
	\caption{Block diagram of the production of composite and component video signals}
	\label{Fig:video_components}
\end{figure}
The signal processing scheme of composite and component video is illustrated in Figure \ref{Fig:video_components}.
As it is depicted, in both cases the basis of video representation is given by the luma-chroma separation of the RGB signals\footnote{Although in case of component video the direct transmission of RGB signals is also allowed in case of UHD video.}.
In the following the actual steps of this production chain is investigated.

\vspace{3mm}
\paragraph{Bandwidth of the video signals:}
Obviously, both the luma and chroma signals are transmitted with finite bandwidth.

As has been already discussed, in case of CRT displays the time domain video signal is drawn on the screen real-time, therefore, the bandwidth of the video signal (defined in $\mathrm{Hz}$) limits the actual horizontal detailedness of the displayed on image.
Thus, the decrease of temporal bandwidth results in the reduction of the spatial, horizontal resolution.
Besides bandwidth efficiency at transmission, the actual choice of video bandwidth has a direct, practical reasons.

The screen of the CRT display is divided both horizontally and vertically to individual RGB pixels.
Therefore, even in case of an continuous CRT driving voltage, the screen itself samples the video signals, that may lead to spatial aliasing artifacts, manifesting in visible Moiré patterns on the screen.
In order to avoid aliasing the video signal has to be temporally bandlimited \textbf{at least} to the half of the spatial sampling frequency, in accordance with the sampling theorem.

The required temporal bandwidth is investigated for the case of the NTSC format:
From table \ref{tab:sd_formats} the number of active lines in the format is $N_{V,A} = 480$, while the active line time is $t_{H,A} \approx 52~\mu \mathrm{s}$.
As the aspect ratio is 4:3 the number of horizontal pixels is approximately $N_{H,A} = \frac{4}{3} N_{V,A} = 640$ (assuming square pixels).
According to the sampling theorem the sine signal with the largest spatial frequency that can be represented with this resolution contains $N_{H,A}$ periods in one line (this is the consecutive ,,black-white-black-white...'' content).
The period time and the frequency of this signal can be calculated as
\begin{equation}
T_{\mathrm{max}} = \frac{t_H}{N_{H,A}/2}, \hspace{1cm} f_{\mathrm{max}} = \frac{N_{H,A}}{2 t_H} = 6.15~\mathrm{MHz}
\end{equation}
which is the theoretical upper frequency limit, based on the Nyquist criterion.

However, experimental results showed that even this theoretical maximal frequency can not be displayed without artifacts, due to the \textbf{Kell effect}.
The Kell effect is caused by the finite size of the CRT's electron beam: instead of the theoretical infinitely narrow beam, which is assumed when applying the sampling theorem, the electron beam has a \textbf{point spread function (PSF)} (i.e. the intensity profile, when projected to a single point on the screen) described approximately by a Gaussian distribution.
\begin{figure}[]
	\centering
	\begin{minipage}[c]{0.65\textwidth}
	\begin{overpic}[width = 0.95\columnwidth ]{figures/kell.png}
	\end{overpic} \end{minipage}\hfill
	\begin{minipage}[c]{0.35\textwidth}	\caption{Illustration of the Kell effect.}
	\label{Fig:Kell}  \end{minipage}
\end{figure}

A simple example is illustrated in Figure \ref{Fig:Kell} for the effect of the beam PSF:
The continuous image to be displayed oscillates vertically at the Nyquist frequency, with alternating black and white lines.
In case the lines of the image exactly coincides with the sampled lines on the screen (i.e. they are aligned) the image can be reconstructed perfectly.
However, if the lines of the image are located between two actual scan lines, due to the non-zero extension of the electron beam PSF the CRT will display the average of the two lines, i.e. a homogeneous grey image is displayed.

As a conclusion, the theoretical maximal spatial frequency can not be displayed on the screen without artifact.
The ratio of the experimental, subjective highest displayable spatial frequency and the theoretical limit is given by the \textbf{Kell factor}, being approximately 0.7 for the NTSC video parameters.

In order to avoid spatial aliasing and by taking the Kell effect into consideration the largest horizontal frequency that can be displayed on the screen without visible artifacts is given by
\begin{equation}
f^{\mathrm{NTSC}}_{\mathrm{max}} = \frac{N_{H,A} \cdot K}{2 t_H} = 4.3~\mathrm{MHz},
\hspace{3mm}
f^{\mathrm{PAL}}_{\mathrm{max}} = \frac{N_{H,A} \cdot K}{2 t_H} \approx 5.2 ~\mathrm{MHz}
\end{equation}
in the NTSC and PAL systems respectively, where the Kell factor is $K = 0.7$.
Therfore, the luma signal (similarly to the early black and white TV signal) is bandlimited to $4.2~\mathrm{MHz}$ in the NTSC and to $5~\mathrm{MHz}$ in the PAL system.

\vspace{3mm}
As discussed earlier, the visual acuity (i.e. the spatial resolution) of the human visual system is less than the half for color information than for luminance.
This property of human vision can be exploited in order to achieve significant video compression.
For the \ycbcr representation this served as the starting point for the subsampling of chroma signals.
In the analog case this phenomena allows the bandwidth reduction of chroma signals, therefore, in component systems the chroma signals are bandlimited to the half of the luma signal (resulting in a halved horizontal color resolution), while in composite video the color information is transmitted with even more reduced bandwidth.

\subsection{Composite video signal}

Analóg átviteltechnika szempontjából a legegyszerűbb megoldás a videójel továbbítására a 3 videókomponens egyetlen érpáron való átvitele.
Ebben az esetben a luma és chroma komponensekből egyetlen ún. \textbf{kompozit} jelet kell képzeni, hogy a vevő oldalon az eredeti három komponens különválasztható.
A feladat megoldására három---alapgondolatában azonos---módszer létezik, az NTSC, PAL és SECAM megoldások.
A rendszerek pontos működésétől eltekintve a következő bekezdés az NTSC és PAL kompozitjelek képzésének alapelvét mutatja be.

A kompozit formátum az NTSC rendszer bevezetésével került kidolgozásra a létező fekete-fehér TV-vevőkkel kompatibilis analóg színes műsorszórás megvalósítására.
A feladat a már létező műsorszóró rendszerben alapsávban továbbított luma jelhez (azaz a fekete-fehér jelhez) a színinformáció olyan módú hozzáadása volt, hogy a létező monokróm vevőkben a többletinformáció minimális látható hatást okozzon, míg a színes vevő megfelelően külön tudja választani a luma és chroma jeleket.
Tehát más szóval a visszafele-kompatibilitás miatt az új színes rendszerben a luma jelet változatlanul kellett átvinni. 
Minthogy az átvitelhez használt RF spektrum jelentős részét már elfoglalták a frekvenciaosztásban küldött egyes TV csatornák (a képinformáció, és az FM modulált hanginformáció), így a luma és chroma komponensek csak ugyanabban a frekvenciasávban kerülhetnek továbbításra.

Az alapsávi fekete-fehér TV jel felépítése egyszerű a már bemutatott \ref{fig:PAL_frame} ábrán látható felépítéssel megegyező:
Egymás után, soronként tartalmazza a CRT elektron-ágyú vezérlőfeszültségének időtörténetét, amely tehát így a műsor vételével teljesen valós időben rajzolja soronként a kijelző képernyőjére az $Y'(t)$ luma jel tartalmát.
Az egyes sorok és képek kijelzése között az elektron-ágyú kikapcsolt állapotban véges idő alatt fut vissza a következő sor, illetve kép elejére. 
Egy fekete-fehér TV sor felépítése az \ref{Fig:PAL_line} ábrán látható.

%
\begin{figure}[]
	\centering
	\begin{minipage}[c]{0.65\textwidth}
	\begin{overpic}[width = 0.95\columnwidth ]{figures/PAL_line.png}
	\end{overpic} \end{minipage}\hfill
	\begin{minipage}[c]{0.35\textwidth}	\caption{Egyetlen TV sor luma jele és szinkron jelei a PAL rendszer időzítései mellett. Az NTSC esetében a TV sor felépítse jellegere teljesen azonos, a PAL-tól eltérő időzítésekkel.}
	\label{Fig:PAL_line}  \end{minipage}
\end{figure}
%

A valós idejű átvitel/kijelzés elvéből látható, hogy a színinformáció átvitele időosztásban sem lehetséges, tehát a chroma jeleket a luma jelekkel azonos frekvenciasávban és időben szükséges átvinni.
A megoldás tárgyalása előtt vizsgáljuk külön a chroma jelek továbbításának módját.

\paragraph{A színsegédvivő bevezetése:}
A színformációt hordozó két chroma jel ($Y'(t)-R'(t), Y'(t)-B'(t)$) egyidőben történő átvitele során alapvető feladat a két analóg jel egyetlen jellé való átalakítása.
Erre az kvadratúra amplitúdómoduláció ad lehetőséget, amely egy olyan modulációs eljárás, ahol az információt részben a vivőhullám amplitúdójának változtatásával, részben annak fázisváltoztatásával kódoljuk (ezzel tehát két független jel vihető át egyszerre). 
Mind PAL, mind NTSC rendszer esetében az emberi látás színekre vett alacsony felbontását kihasználva a chroma jeleket erősen (PAL esetében pl. a luma jel ötödére, $1~\mathrm{MHz}$-re) sávkorlátozzák, ezzel az apró, nagyfrekvencián reprezentált részleteket kisimítják. 
Ezután a kvadratúramodulált chroma jeleket pl. PAL esetén
\begin{equation}
c^{\mathrm{PAL}}(t) = \underbrace{U'(t)}_{\left( B'- Y'\right) / 2.03} \cdot \sin \omega_c t + \underbrace{V'(t)}_{\left( R'- Y'\right) / 1.14}  \cdot \cos \omega_c t
\label{Eq:PAL_cr}
\end{equation}
alakban állíthatjuk elő, ahol $\sin \omega_c t$ az ún. \textbf{színsegédvivő}, $\omega_c$ a színsegédvivő frekvencia, $U'(t)$ az ún. fázisban lévő, $V'(t)$ pedig a kvadratúrakomponens.
A kvadratúramodulált színjelek tehát egyszerűen az átskálázott színkülönbségi jelek fázisban és kvadratúrában lévő színsegédvívővel való modulációjával állítható elő.

A színjelek demodulációja koherens (fázishelyes) vevővel egyszerű alapsávba való lekeveréssel és aluláteresztő szűréssel valósítható meg:
\begin{align}
\begin{split}
\sin x \cdot \sin x = \frac{1-\cos 2x}{2}&,\hspace{1cm}
\cos x \cdot \cos x = \frac{1+\cos 2x}{2} \\
\sin x &\cdot \cos x = \frac{1}{2}\sin 2x
\end{split}
\end{align}
trigonometrikus azonosságok alapján $U'(t)$ demodulációja
\begin{multline}
c^{\mathrm{PAL}}_{\mathrm{QAM}}(t)\cdot \sin \omega_c t = U'(t)\cdot \sin \omega_c t\cdot \sin \omega_c t + V'(t) \cdot \cos \omega_c t  \cdot	\sin \omega_c t = \\
\frac{1}{2} U'(t) -
\underbrace{ \xcancel{ \frac{1}{2} U'(t)\cos 2 \omega_c t  + V'(t) \cdot \frac{1}{2}\sin 2 \omega_c t }}_{\text{aluláteresztő szűrés}}
\end{multline}
szerint történik, míg $V'(t)$ demodulálása hasonlóan $\sin \omega_c t$ lekeverés szerint.
A megfelelő demodulációhoz tehát a vevőben a színsegédvivő fázishelye, koherens előállítása elengedhetetlen.
\begin{figure}[]
	\centering
	\hspace{4mm}
	\begin{overpic}[width = 0.70\columnwidth ]{figures/QAM_mod_demod.png}
	\end{overpic}
	\caption{QAM moduláció és demoduláció folyamatábrája}
	\label{Fig:QAM_mod_demod}
\end{figure}

Az NTSC rendszerben a PAL-hoz hasonlóan a színjelek
\begin{equation}
c^{\mathrm{NTSC}}_{\mathrm{QAM}}(t) = I'(t) \cdot \sin \omega_c t + Q'(t) \cdot \cos \omega_c t
\end{equation}
alakban kerültek átvitelre, ahol az in-phase és kvadratúra komponensek rendre
\begin{align}
\begin{split}
I'(t) &= k_1 (R'-Y') + k_2 (B'-Y) ,\\ 
Q'(t) &= k_3 (R'-Y') + k_4 (B'-Y).
\end{split}
\end{align}
A $k_{1-4}$ konstansokat úgy választották meg, hogy az in-phase és kvadratúra modulált jelek nem közvetlenül a kék és piros merőleges bázisvektorok (ld. \ref{Fig:ycbcr_gamut} ábra), hanem ezek kb. $+20^{\circ}$ elforgatottja.
Az így kapott új tengelyek a magenta-zöld és türkiz-narancssárga tengelyek a közvetlen modulálójelek.
Ennek oka, hogy úgy találták, az emberi látás felbontása jóval nagyobb türkiz-narancssárga közti változásokra, mint a magenta-zöld között.
Ezt kihasználva a magenta-zöld $Q'(t)$ színjeleket az $I'(t)$ jelhez képest is jobban sávkorlátozták, sávszélesség-takarékosság céljából.
A PAL rendszer bevezetésének idejére azonban kiderült, hogy ez rendszer felesleges túlbonyolítása, így az új rendszerben megmaradtak az eredeti színkülönbségi jelek modulációjánál.

\vspace{3mm}
Vizsgáljuk végül a modulált színjel fizikai jelentését, az egyszerűség kedvéért $c^{\mathrm{PAL}}(t)$ esetére (PAL rendszerben)!
Az \eqref{Eq:PAL_cr} egyenlet egyszerű trigonometrikus azonosságok alapján átírható a 
\begin{equation}
c^{\mathrm{PAL}}_{\mathrm{QAM}}(t) = \sqrt{U'(t)^2 + V'(t)^2} \, \sin \left( \omega_c t + \arctan \frac{V'(t)}{U'(t)} \right)
\end{equation}
polár alakra.
Minthogy a moduláló $U',V'$ jelek a színkülönbségi jelekkel arányosak, így a fenti kifejezést \eqref{eq:saturation_1} és \eqref{eq:hue}-val összehasonlítva megállapítható, hogy a QAM modulált jel egy olyan szinuszos vivő, amelynek pillanatnyi amplitúdója a továbbított színpont telítettségét, pillanatnyi fázisa a színpont színezetét adja meg.

\begin{figure}[]
	\centering
	\hspace{4mm}
	\begin{overpic}[width = 0.50\columnwidth ]{figures/SMPTE_Color_Bars.png}
\small
\put(-7	,0){(a)}
	\end{overpic} \hfill
	\begin{overpic}[width = 0.395\columnwidth ]{figures/vectorscope.png}
\small
\put(-10,0){(b)}
	\end{overpic}
	\caption{Egy gyakran alkalmazott vizsgálókép (SMPTE color bar) (a) és vektorszkóppal ábrázolva (b).}
	\label{Fig:bar_pattern_vscope}
\end{figure}

A színsegédvivő amplitúdójának és fázisának egyszerű értelmezhetősége miatt az NTSC és PAL jeleket gyakran vizsgálták ún. vektorszkóp segítségével jól meghatározott vizsgálóábrák megjelenítése mellett.
A vektorszkóp kijelzője gyakorlatilag a \ref{Fig:ycbcr_gamut} ábrán is látható $B'-Y', B'-Y'$ térben jeleníti meg a teljes képtartalom (azaz egyszerre az összes képpont) chroma jeleit, $Y'$-tól függetlenül a demodulált chroma-jelek megjelenítésével.
A vektorszkóp gyakorlatilag egy olyan oszcilloszkóp, amelynek $x$ kitérését a demodulált $B'-Y'$, $y$-kitérést a demodulált $R'-Y'$ jel vezérli, így a teljes képtartalom színezetét szinte egyszerre jeleníti meg az előre felrajzol vizsgálati rácson.
Egy tipikus vizsgáló ábra és annak vektorszkópos képe látható a \ref{Fig:bar_pattern_vscope} ábrákon.
A vektorszkóp alkalmazásának előnye, hogy az esetleges amplitúdó és fázishibából származó telítettség és színezethibák jól láthatóvá válnak a kijelzőn az egyes felvetített pontok ''összeszűkülése/tágulása'', illetve a teljes konstelláció elfordulásaként.
Megjegyezhető, hogy a mai digitális videojeleket is gyakran ábrázolják szoftveres vektorszkópon az egyes pixelek színezetének vizsgálatához.

\paragraph{A színsegédvivő frekvencia:}
Vizsgáljuk most, hogyan választható meg a színsegédvivő $\omega_c$ vivőfrekvenciája úgy, hogy a QAM modulált $c^{\mathrm{PAL}}(t)$ jelet a luma jelhez hozzáadva a vevő oldalon lehetséges legyen a vett $c^{\mathrm{PAL}}(t) + Y'(t)$ jelből az eredeti chroma és luma jelek szétválasztása!

A jelek vevőoldali szétválasztására a luma és chroma jelek spektruma ad lehetőséget:
Láthattuk, hogy a videójel az egyes TV sorokban megjelenítendő világosság és színinformáció sorfolytonos időtörténeteként fogható fel.
Természetes képeken a képtartalom sorról sorra csak lassan változik (természetesen a képtartalomban jelenlévő vízszintes éleket leszámítva), így mind a luma, mind a chroma jelek ún. kvázi-periodikusak, azaz közel periodikusak.
Jel- és rendszerelméleti ismereteink alapján tudjuk, hogy egy periodikus jel spektruma vonalas, a jelfrekvencia egész számú többszörösein tartalmaz csak komponenseket.
Ennek megfelelően mind a luma, mind a chroma jelek spektruma közel vonalas: az energiájuk a sorfrekvencia egész számú többszörösein csomósodik.
Természetesen a luma jel az alapsávban helyezkedik el ($0~\mathrm{Hz}$ környezetében), kb. $5.6~\mathrm{MHz}$ sávszélességben\footnote{Ez a sávszélesség eredményezi az azonos horizontális és vertikális képfelbontást.}.
A QAM modulált chroma jel spektruma a sávkorlátozás miatt keskenyebb ($1~\mathrm{MHz}$), és középpontját $\omega_c$ vivőfrekvencia határozza meg.
\begin{figure}[]
	\centering
	\hspace{4mm}
	\begin{overpic}[width = 0.80\columnwidth ]{figures/LC_interlace.png}
	\end{overpic} \hfill
	\caption{A luma és chroma jelek spektrális közbeszövésének alapelve a teljes spektrumokat ábrázolva (a) és a spektrális csomókat felnagyítva (b)}
	\label{Fig:YC_interlace}
\end{figure}

A luma-chroma jel összegzése ennek ismeretében egyszerű: 
Az $\omega_c$ vivőfrekvencia megfelelő megválasztásával elérhető, hogy a chroma jel spektrumvonalai (spektrumcsomói) éppen a luma jel spektrumvonalai közé essen, azaz a spektrumukat átlapolódás nélkül közbeszőhetjük.
Az eljárás alapötletét \ref{Fig:YC_interlace} ábra illusztrálja $f_{\mathrm{H}}$-val a sorfrekvenciát jelölve.
A szétválaszthatóság feltétele ekkor 
\begin{equation}
f_c = f_{\mathrm{H}} \cdot \left( \mathrm{n} + \frac{1}{2}\right), \hspace{1.5cm} \mathrm{n} \in \mathcal{N} 
\end{equation}
azaz a színsegédvivő frekvenciája a sorfrekvencia felének egész szűmú többszörösének kell, hogy legyen \footnote{Megjegyezhető, hogy PAL esetében az előre adott sorfrekvenciához egyszerű volt a színsegédvivő-frekvencia megválasztása, míg NTSC esetén bizonyos okok miatt a sorfrekvencia és ebből következően a képfrekvencia megváltoztatására volt szükség. 
Innen származnak a ma is használatos $59.94$ és $29.97~\mathrm{Hz}$ képfrekvenciák, amelyeket a következő fejezet tárgyal részletesen.}.

\paragraph{A CVBS kompozit videójel és luma-chroma szétválasztás:}
Ennek ismeretében végül a teljes kompozitjel a 
\begin{equation}
\text{CVBS}(t) = \mathrm{Sy}(t) + Y'(t) + c_{\mathrm{QAM}}(t)
\end{equation}
alakban áll elő, ahol $Y'$ a luma jel, $c_{\mathrm{QAM}}$ a QAM modulált chroma jelek és $\mathrm{S\!y}(t)$ a kioltási időben jelen lévő sorszinkron és képszinkron jelek.
A CVBS elnevezés gyakori szinoníma a kompozit videójelre, jelentése C: color, V: video (luma), B: blanking (azaz kioltás) és S: sync (azaz szinkronjelek).

Az így létrehozott videójel a fekete-fehér képhez képest csak a modulált színsegédvivőt tartalmazza többletinformációnak.
Egyszerű fekete-fehér vevőn a CVBS jelet megjelenítve a színinformáció nagyfrekvenciás zajként, pontozódásként (ún. \href{http://www.techmind.org/colrec/}{chroma dots}) jelenik csak meg a kijelzőn, így a visszafelé kompatibilitás biztosítva volt.
Színes vevőkben a CVBS jelből a luma és chromajel elméletileg fésűszűréssel szeparálható a sorfrekvencia felének egész számú többszöröseit elnyomva.
Ez ideálisan egy soridejű késleltetést igényel \footnote{A bizonyításhoz vizsgáljuk $h(t) = \delta(t) + \delta(t-t_{\mathrm{H}})$ szűrő átviteli karakterisztikáját, amely szűrő a jelből kivonja $t_H$-val késleltetett önmagát!}.
A fésűszűrős luma-chroma szeparáció lehetősége már az NTSC fejlesztésének idején ismert volt, azonban a szükséges soridejű késleltető nem állt rendelkezésre, ezért a korai NTSC vevők egyszerű alul/felüláteresztő szűrőkkel, vagy egyszerű chroma jelre állított lyukszűrőkkel szeparálták a luma-chroma jeleket.
Ennek eredményeképp még a színes vevőkben is a chroma jelen kisfrekvenciás tartalomként jelen lehetett a világosságinformáció látható \href{https://en.wikipedia.org/wiki/Dot_crawl}{hatással a megjelenített képre}.
A megfelelő analóg PAL fésűszűrő-tervezés még a 90-es években is aktív \href{https://www.renesas.com/in/en/www/doc/application-note/an9644.pdf}{K+F} alatt álló terület volt.

\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.45\columnwidth ]{figures/ntsc_color_line.png}
	\end{overpic} \hfill
	\begin{overpic}[width = 0.48\columnwidth ]{figures/Waveform_monitor.jpg}
	\end{overpic} \hfill
	\caption{Az SMPTE color bar vizsgáló ábrának egy, illetve két sorának hullámformája sematikusan (a), és egy hulláforma monitoron (b) vizsgálva}
	\label{Fig:NTSC_line}
\end{figure}

Az elmondottak alapján az NTSC rendszerben a \ref{Fig:bar_pattern_vscope} ábrán látható vizsgálóábrának egy sorának kompozit ábrázolását a \ref{Fig:NTSC_line} mutatja be jellegre helyesen, és egy konkrét hullámforma monitoron mérve.
Az ábrán megfigyelhető az egyes oszlopokhoz tartozó hullámalak: látható, hogy a csökkenő világosságú oszlopokra (amelyek világosságát szaggatott vonal jelzi) hogyan ültették rá a QAM modulált chroma jeleket.
Az első és utolsó fehér, illetve fekete oszlop esetén a chroma jelek amplitúdója zérus (fehérpont), egyéb esetekben a szinuszos színsegédvivő amplitúdója az oszlopok színének telítettségével, fázisa a színezetükkel arányos.
Megjegyezzük, hogy a tényleges hullámforma már átskálázott chroma jeleket ábrázol, amely átskálázás épp azért történik, hogy a teljes CVBS jel beleférjen a fizikai interface dinamikatartományába (ez természetesen a nagy telítettségű színek esetén okozna problémát).
Ez magyarázza tehát az eddig figyelmen kívül hagyott 2.03 és 1.14 skálafaktorokat pl. \eqref{Eq:PAL_cr} esetében.


Az NTSC jel felépítése alapján egyértelmű, hogy a megfelelő színek helyreállításához a vevőben a színsegédvivő fázisának nagyon pontos ismerete szükséges.
Ahhoz, hogy ez biztosítva legyen a sorkioltási időben az ún. hátsó vállra (ld. \ref{Fig:PAL_line} ábra) beültetésre került néhány periódusnyi (9) képtartalom nélküli referenciavivő, az ún. color burst, vagy burst jel.
Ez a burst jel megfigyelhető a \ref{Fig:NTSC_line} ábrán is.

Ennek ellenére az NTSC rendszer továbbra is fázisérzékeny volt, hiszen fázishibát a vevőben is bármelyik alkatrész okozhatott.
A QAM moduláció jelege miatt már a legkisebb fázishiba is látható színezetváltozást okozott a megjelenített képen.
A PAL rendszer tervezésének egyik fő célja épp ezért a rendszer fázishibára vett érzékenységének csökkentése volt

\paragraph{A PAL rendszer:}
Míg az egyszerű NTSC rendszer már 1953-ban bevezetésre került Amerikában, addig Európában egészen az 1960-as éveikg vártak a színes műsorszórás bevezetésére.
Ennek oka, hogy az eltérő hálózati frekvencia miatt az NTSC-t nem lehetett egy az egyben átemelni Európába (ld. később).
Mire az európai rendszert kifejlesztették, az NTSC rendszer jó néhány gyengeségére fény derült, így az újonnan kifejlesztett PAL (Phase Alternate Lines) ezek kijavítását célozta főként meg.
Ennek eredményeképp a PAL rendszer más QAM modulációval dolgozik (a chroma jelek közvetlenül a modulálójelek), eltérő a színsegédvivő frekvencia, és legfontosabb újításként: egy egyszerű megoldással szinte érzéketlen a fázishibára.
\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.45\columnwidth ]{figures/PAL1.png}
	\end{overpic} \hfill
	\begin{overpic}[width = 0.45\columnwidth ]{figures/PAL2.png}
	\end{overpic} \hfill
	\caption{Az SMPTE color bar vizsgáló ábrának egy, illetve két sorának hullámformája sematikusan (a), és egy hulláforma monitoron (b) vizsgálva}
	\label{Fig:PAL1}
\end{figure}

Láthattuk, hogy a vevő oldalán bármilyen fázishiba a színezet jól látható torzulását okozza.
Mivel a fázishiba gyakran elkerülhetetlen, ezért hatásának kiküszöbölésére a PAL rendszer a következő egyszerű megoldást alkalmazza:
\begin{itemize}
\item Az adó oldalon (a PAL jel létrehozása során) képezzük QAM moduláció során a V' chromajel előjelét minden második TV-sorban negáljuk meg, azaz sorról sorra fordított előjellel vigyük át (ez ekvivalens a sorról sorra változó $\pm \cos \omega_c t$ vivővel való modulációval)!
Az eljárás szemléltetésére tegyük fel, hogy két egymás utána sorban minden horizontális pozícióban a színinformáció azonos.
Ekkor egy adott pontra az n. és (n+1). sorban átvitt $U',V'$ jeleket a \ref{Fig:PAL1} (a) ábra szemlélteti pl egy lila képpont átvitele esetén.
\item Tegyük fel, hogy a vevő oldalon a vett jelhez $\Delta \alpha$ fázishiba adódik az átvitel és demoduláció során.
Természetesen a fázishiba hatására az így vett színvektor mind az n., mind az (n+1). sorban azonos irányba fordul az $U'-V'$ konstellációs diagramon (azaz a $R'-Y', B'-Y'$ síkon), ahogy az a \ref{Fig:PAL1} (b) ábrán látható.
\item A vevő oldalán forgassuk vissza minden második sorban a vett $V'$ komponens előjelét és képezzük az (n+1). sor és az n. sor átlagát.
Ezzel természetesen a színjelek vertikális felbontását csökkentjük (az átlagképzés az apró részleteket elsimítja), azonban ennek eredménye az emberi szem színezetre vett felbontása eredményeképp az információveszteség nem látható (a horizontális felbontás már egyébként is jelentősen lecsökkent az egyszerű sávkorlátozás hatására).
Könnyen belátható, hogy a két vektor átlagát képezve éppen az eredeti, hibamentes színvektort kapjuk eredményül.
Két sor esetén azonos sortartalom esetén tehát ezzel az egyszerű trükkel a fázishiba hatása teljesen kiküszöbölhető, míg levezethető, hogy változó sortartalom esetén a fázishiba az átlagvektor hosszának csökkenését okozza, tehát színezetváltozás helyett csak telítettségváltozást okoz.
\end{itemize}
A bemutatott módosított modulációs módszerrel még aránylag nagy fázishibák hatása is minimális hatással van a megjelenített képre.
Az ok, hogy mégis több, mint egy évtizedet kellett várni a PAL rendszer bevezetésére az volt, hogy a módszer alkalmazásához (az átlagolás elvégzéséhez) a videójel soridejű késleltetésére volt szükség.
Ez az 50-es években analóg módon nem megoldható probléma volt amely a PAL implementálását hátráltatta.

A PAL bevezetését végül az olcsón tömeggyártható ún akusztikus művonalak megjelenése tette lehetővé.
Ez az akusztikus művonal, vagy \href{https://www.google.com/search?q=PAL+delay+line&client=firefox-b-d&sxsrf=ALeKk03EUTzVwc7dkYJFnEK-nlEI_p3hng:1586379019108&source=lnms&tbm=isch&sa=X&ved=2ahUKEwi90Kav2tnoAhXJ-ioKHWz6AJcQ_AUoAXoECA0QAw&biw=1407&bih=675}{PAL delay line} egy egyszerű üvegtömb, amelyre egy piezo aktuátor és piezo vevő csatlakozik.
Az adó a TV chroma jelével arányos mechanikai rezgéseket (ultrahang) \href{https://www.youtube.com/watch?v=-qerYLM-eEg}{hoz létre}, amely többszörös visszaverődések után épp egy soridőnyi késleltetést szenvedve ér a vevő elektródához.
Az ultrahang alapú késleltetővonalak egészen a 90-es évek végéig a PAL dekóderek részét képezték.


\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.82\columnwidth ]{figures/PAL_coder.png}
	\end{overpic} \hfill
	\caption{A PAL kódoló felépítése}
	\label{Fig:PAL_coder}
\end{figure}
Az egyszerű PAL kódoló felépítése az eddig elmondottak alapján a \ref{Fig:PAL_coder} ábrán látható.
Röviden összefoglalva, mind a PAL, mind NTSC esetén a kompozitjel létrehozása során a feladat a Gamma-torzított $R',G',B'$ jelekből az $Y',U',V$ (PAL) és $Y',I',Q'$ (NTSC) jelek létrehozása, majd az $U',V'$ és $I'Q'$ jelek megfelelő QAM modulációja. 
Az így létrehozott jeleket összeadva és a kioltási időben továbbított szinkronjelekkel ellátva előáll a CVBS kompozit jel.

\vspace{3mm}
A kompozit videójel fizikai interface megvalósítása szabványról szabványra változó.
Konzumer felhasználás (pl. kézikamerák, videólejátszók, DVD lejátszók) szempontjából a legelterjedtebb csatlakozó a sárga jelölésű RCA végződés, amely az esetleges kísérő hangtól szigetelve, külön érpáron továbbítja a kompozit videójelet.
\begin{figure}[]
	\centering
	\begin{minipage}[c]{0.6\textwidth}
	\begin{overpic}[width = 0.45\columnwidth ]{figures/Composite-video-cable.jpg}
	\end{overpic} 
		\begin{overpic}[width = 0.45\columnwidth ]{figures/s_video.jpg}
	\end{overpic} \end{minipage}\hfill
	\begin{minipage}[c]{0.4\textwidth}
	\caption{Konzumer alkalmazásokhoz használt sárga jelölésű RCA csatlakozó (a) és a luma-QAM chroma jeleket külön érpáron átvivő S-videó csatlakozó (b)}
	\label{Fig:composite_video}  \end{minipage}
\end{figure}

A kompozit és komponens jelek közti kompromisszumként az S-video formátum a luma és chroma jeleket külön érpáron viszi át.
Ezt leszámítva az interface jele teljesen a kompozit videóval azonosak, továbbíthat akár NTSC, akár PAL (akár SECAM) videókomponenseket:
A luma tehát változatlanul alapsávban, míg a chroma a színsegédvivővel modulálva kerül átvitelre.
A chroma jelek modulációja elkerülhetetlen, hiszen a két független színkülönbségi jel egy érpárra való ültetéséhez azokat legalább a sávszélességükkel megegyező frekvenciájú vivőjellel való moduláció szükséges az átlapolódás elkerüléséhez.
Az S-video szabvány csatlakozója a \ref{Fig:composite_video} (b) ábrán látható.


\subsection{Component video signal}

The idea of transmitting the video signal by separated components is straightforward, still, it was allowed by technology only decades after composite video was introduced:
While interfaces for device-to-device video transmission could be achieved even with analog data, the broadcasting of component video could be only resolved with the introduction of digital video standards.
In case of component video the transmitted signals are directly the luma and the chroma signals (or less often the $R'G'B'$ signals), or more precisely the \ypbpr components.

The \ypbpr representation consists of the three following signals:
\begin{itemize}
\item $Y'$: the luma component, calculated by the luma coefficients defined by the RGB primaries.
The required sync pulses are added to the luma component.
Therefore, a device with composite video output can be connected to the $Y'$ interface of a display with component input.
As a result the black and white image is displayed with the modulated chroma signal resulting in high frequency noise (,,chroma dotting'') on the screen.
\item $P'_{\mathrm{B}},P'_{\mathrm{R}}$: the $B'-Y', R'-Y'$ chroma components, rescaled to the actual physical interface, usually requiring the dynamic range of $\pm 0.5~\mathrm{V}$.
Notation $P$ is the legacy of the composite systems, where color information is carried by the \textbf{P}hase of the subcarrier.
\end{itemize}
\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.45\columnwidth ]{figures/1280px-Component-cables.jpg}
	\end{overpic} \hfill
	\begin{overpic}[width = 0.45\columnwidth ]{figures/YPBPR_signals.png}
	\end{overpic} \hfill
	\caption{RCA connectors, applied frequently for the physical interface of component video (a) and the component video signals of the bar test pattern (b)}
	\label{Fig:comp_video}
\end{figure}
Similarly to composite video, the actual parameters of the component formats depend on the region of application: 
components formats can be considered as the transmission of NTSC and PAL video formats without the QAM modulation of the chroma signals.
Therefore, both 625 and 525 scan line interlaced component formats exist with $60~\mathrm{Hz}$ and $50~\mathrm{Hz}$ field rate.
A simple example is depicted for the separately transmitted \ypbpr signals in case of the SMPTE color bar test pattern is depicted in Figure \ref{Fig:comp_video} (b).

In consumer electronic the component video interface is equipped with RCA connectors, with the red and blue cables carrying the red and blue chroma signals and the green connector transmitting the luma information.

\hspace{3mm}
Finally, from the numerous analog video interfaces two most widespread are mentioned here:
\begin{itemize}
\item The SCART connector (or EuroSCART, shorthand for Syndicat des Constructeurs d'Appareils Radiorécepteurs et Téléviseurs) was designed for the transmission of bidirectional composite, S-video and RGB components in the same time, with also carrying stereo audio and digital signing signals in the same time.
Before the advent of the HDMI interface SCART connectors also allowed the transmission of high definition 1080p format via \ypbpr component signals.
The typical 21-pin SCART connector is depicted in Figure \ref{Fig:scart_vga} (a).
\item The VGA (Video Graphics Array) connector is still a commonly used analog component interface between video cards and displays (external monitors, projectors, etc.).
The VGA interface transmits analog RGB components (in the color space of the video card) along with dedicate vertical and horizontal sync (VSYNC and HSYNC) cables.
Nowadays the VGA interface is superseded with DVI, HDMI and DisplayPort digital interfaces.
\end{itemize}

\begin{figure}[]
	\centering
	\begin{minipage}[c]{0.63\textwidth}
	\begin{overpic}[width = 0.47\columnwidth ]{figures/scart.jpg}
	\end{overpic} \hfill
		\begin{overpic}[width = 0.4\columnwidth ]{figures/vga.jpg}
	\end{overpic} \end{minipage}\hfill
	\begin{minipage}[c]{0.35\textwidth}
	\caption{The analog composite/component SCART (a) and RGB component VGA (a) connectors}
	\label{Fig:scart_vga}  \end{minipage}
\end{figure}

\section{Digital video formats}
	
\subsection{The SD format}

The first digital video format was developed by the ITU (International Telecommunication Union) and published in Rec. 601 (or ITU-601) in 1982\footnote{The CCIR (the predecessor of ITU) received the 1982–83 Technology and Engineering Emmy Award for its development}.

The SD format is basically the digital representation of the component analog formats, discussed in the foregoing:
The structure of digital video is obtained by the sampling of the analog video signal, as depicted in Figure \ref{fig:PAL_frame}, containing all the blanking intervals besides the active video content.
Therefore, digital video is the direct digitized version of the \ypbpr signals.
As already discussed in the previous chapter, the components of the video format are the digital representation of the color pixels, referred to as the \ycbcr components.
The signal processing scheme of digital video representation is illustrated in Figure \ref{Fig:SD_production}.
\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.8 \columnwidth ]{Figures/YCbCr_production.png}
	\end{overpic}
	\caption{Production of SD video format by the digitization of \ypbpr components}
	\label{Fig:SD_production}
\end{figure}

The digitization of the analog video signal consists of two main steps:
\begin{itemize}
\item sampling of the continuous video signal with a-priori antialiasing filtering
\item quantization of the continuous signal levels to discrete codes
\end{itemize}
The questions of quantization, i.e. digital representation of the individual pixels has been already discussed in the previous chapter.
Still, the parameters of sampling---or more specifically the sampling frequency---needs to be specified, defining the number of pixels per line, which, along with the number of lines gives the spatial resolution of the digital format.

\paragraph*{Sampling frequency of the video signal:\\}
The sampling frequency of the analog video components was chosen based on the following considerations:
\begin{itemize}
\item For several decades analog formats varied between from region to region due to the co-existence of NTSC, PAL and SECAM systems.
As a straightforward endeavour, a basic goal was to find a common sampling frequency, compatible with all the existing analog formats.
Furthermore, orthogonal sampling was an obvious need, meaning that in each system a single scan line should contain an integer number of samples (pixels).
Mathematically these requirements can be formulated, as the line period time has to be dividable by the sampling period, and equivalently
the sampling frequency has to be the multiple of the line frequency both in the PAL and the NTSC system, satisfying
\begin{equation}
f_s = n \cdot f_H^{\mathrm{PAL}} = m \cdot f_H^{\mathrm{NTSC}},
\end{equation}
%
where $n, m$ are integers.
The line frequencies in the two systems are given by
\begin{align}
f_H^{\mathrm{PAL}} &= 25 \cdot 625 = 15625~\mathrm{Hz} \\
f_H^{\mathrm{NTSC}} &= 30 \cdot \frac{1000}{1001} \cdot 525 = 15734.2~\mathrm{Hz} ,
\end{align}
with the smallest common multiple being
\begin{equation}
144 \cdot f_H^{\mathrm{PAL}} = 143 \cdot f_H^{\mathrm{NTSC}} = 2.25~\mathrm{MHz}.
\end{equation}
Therefore, the sampling frequency must be chosen as the multiple of $2.25~\mathrm{MHz}$.
\item On the other hand, according to the sampling theory, the sampling frequency has to be at least twice the bandwidth of the sampled signal in order to avoid aliasing artifacts.
In the foregoing it was discussed that---by taking also the Kell effect into consideration---the bandwidth of the luma signal is around $5.6~\mathrm{MHz}$, with the chroma bandwidth being the half of it.
\end{itemize}
The smallest frequency satisfying both requirements is given by 
\begin{equation}
f^{\mathrm{Y,SD}}_s = 13.5~\mathrm{MHz},
\end{equation}
being chosen as the sampling frequency of the luma signal of standard definition video.
As the chroma signals are bandlimited to the half of luma signal due to the lower visual acuity of the HVS, therefore, the chroma signals are sampled with halved sampling frequency ($f^{\mathrm{Ch,SD}}_s = 6.75~\mathrm{MHz}$).

The number of pixels in a scan line---given by the line period time divided by the sampling interval---is $N_{H,t} = \frac{T_H}{1/f_s} = 858$ pixels in the NTSC and $864$ in the PAL system, including the horizontal blanking intervals as well.
%
\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.65 \columnwidth ]{Figures/SD_formats.png}
	\end{overpic}
	\caption{The SD video format in the American and European systems}
	\label{Fig:SD_format}
\end{figure}
For the sake of further unification of the standard definition format the number of active pixels in a line was chosen to be 720 pixels in both the American and European systems.
Note that the active line period is $52~\mu \mathrm{s}$ in both systems, from which the number of active pixels is $N_{H,a} = T_{\mathrm{H,a}} \cdot f_s = 702$.
Hence, the 720 pixels contains a small interval from the blanking time as well, due to the uncertainty in the position of the actual analog video signal, distortions and smears near the edges, originating from analog processing.
The actual digital video data is taken as 704 pixels from the center of the nominal 720 pixels.
Obviously, the number of the vertical total and active pixels are simply given by the number of total and active scan lines.

The spatial resolution/pixel number of the SD formats is illustrated in \ref{Fig:SD_format}.
The two formats are named after the number of the active lines, resulting the the two SD formats being \textbf{480i} and \textbf{576i}, with ,,i'' referring to the fact that for both formats only interlaced scan is defined.

As it has been already discussed, the aspect ratio of the reproduced SD images should have the aspect ratio of 4:3, which is termed as the \textbf{display aspect ratio (DAR)}.
However, the ratio of the number of the horizontal and vertical pixel numbers---termed as the \textbf{storage aspect ratio (SAR)} does not coincide this ratio in neither formats.
The target display aspect ratio, therefore, can be achieved only by applying non-square pixels on the display side, described by the \textbf{pixel aspect ratio (PAR)}, defined as the ratio of the horizontal and vertical pixel size.
Therefore, format 480i has a pixel aspect ratio of 10:11, and 576i has the PAR of 12:11 with the DAR of 4:3 for both cases.
In contrary, computer displays are employing squared pixels with the PAR of 1:1, and the standard VGA resolution being 640x480 pixels.

\begin{table}[h!]
\caption{Spatial and temporal parameters of SD video formats}
\renewcommand*{\arraystretch}{2}
\label{tab:SD_formats}
\begin{center}
    \begin{tabular}[h!]{ @{}c | | l | l | l @{} }%\toprule
				         &   480i	   &    576i \\ \hline
    active pixel number: &  704 x 480  &   704 x 576   \\
    total pixel number:  &  858 x 525  &  864 x 625 \\
    display aspect ratio:&  4:3 &  4:3 \\
    pixel aspect ratio   &  10:11  & 12:11  \\
    field rate:          &  $59.94~\mathrm{Hz}$ &   $50~\mathrm{Hz}$ \\
    frame rate:          &   $29.97~\mathrm{Hz}$ &  $25~\mathrm{Hz}$ \\
    \end{tabular}
\end{center}
\end{table}

As a short summary of the foregoing the properties of the SD formats are the following:
\begin{itemize}
\item The chromaticity of the primaries and the gamut of the device RGB color space is illustrated in Figure \ref{Fig:SD_gamut} (a).
The white point of the colors space is D65 white.
The luma is calculated from the nonlinear RGB components as 
\footnote{
It is noted here that the above coefficients---unmathematically---do not coincide with the second row of the $XYZ \rightarrow RGB$ transformation matrix (which can be calculated from the primaries and the white point).
This means that strictly speaking the $Y$ component---calculated from the RGB values with the above coefficients---does not equal to the relative luminance.
Instead, for the sake of simplicity the luma coefficients of the NTSC system were used.
These mathematical inconsistencies are usual in videotechnologies.}
\begin{equation}
Y' = 0.299 R' + 0.587 G' + 0.112 B'.
\end{equation}
\item In order to achieve perceptual quantization the source RGB signals are pre-distorted with the opto-electronic transfer function, given by
%
\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.4 \columnwidth ]{Figures/sd_gamut.png}
	\small
	\put(0,0){(a)}
	\end{overpic}
	\hspace{2mm}
	\begin{overpic}[width = 0.55 \columnwidth ]{Figures/sd_OETF.png}
	\small
	\put(0,0){(b)}
	\end{overpic}
	\caption{Gamut (a) and OETF (b) of the ITU-601 SD format}
	\label{Fig:SD_gamut}
\end{figure}
\begin{equation}
E = 
\begin{cases}
4.500 L, \hspace{20mm} \mathrm{ha}\, L < 0.018 \\
1.099 L^{0.45} - 0.099, \hspace{3mm} \mathrm{ha}\, L \geq 0.018,
\end{cases}
\end{equation}
where $L \in \{ R, G, B \}$.
The entire curve can be approximated by $L^{0.5}$.
\item The original SD display aspect ratio was 4:3.
After the introduction of the HD format also the SD format was extended with the aspect ratio of 16:9.
\item SD format exclusively supports interlaced scanning.
\item ITU-601 originally only allowed the chroma subsampling scheme 4:2:2.
Later it was extended with the scheme 4:2:0 for consumer applications.
\item The SD format applies the sampling frequency of $f_s = 13.5~\mathrm{MHz}$. 
The format strictly prescribes the antialiasing low-pass filters, bandlimiting the luma signal to $5.75~\mathrm{MHz}$ as the upper frequency limit and $6.75~\mathrm{MHz}$ as the lower limit of the stopband.
The chroma is sampled with halved sampling frequency, and prefiltered with corresponding antialiasing filters.
\item The sampled luma and chroma sampled are quantized with 8 (for consumer electronics and broadcasting applications) or 10 bits (in studio applications) of bit depths.
\end{itemize}

\subsection{The HD format}

The starting point for the introduction of the analog video formats, leading directly to SD digital format was to cover approximately about $10^{\circ}$ of the central vision from the viewer's field of view.
Obviously, this requirement does not directly define the display size, or the spatial resolution.
Instead at a given resolution and display size the optimal viewing distance can be derived.
In the following, first this optimal viewing distance is investigated, highlighting the basic motivation behind the introduction of the HD and UHD formats.

\subsubsection*{The optimal viewing distance}

\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.67 \columnwidth ]{Figures/hd_pixel_angle_mod.png}
	\small
	\end{overpic}
	\caption{Geometry for deriving the optimal viewing distance}
	\label{Fig:optimal_vd}
\end{figure}

Generally speaking the goal of pixel based image reproduction is to ensure that the light rays, arriving from adjacent pixels include an angle below the spatial resolution of the human vision.
Once it is achieved it is inherently ensured that the pixel structure of the image is not visible, and allows RGB based color reproduction, since instead of perceiving the individual RGB primaries, only the mixture of the primaries is perceived.
In the foregoing it has been discussed that the visual acuity of human vision is $\frac{1}{60}^{\circ}$ (at least for luminance, for colors the resolution is significantly lower), from which for a given pixel size the minimal viewing distance can be derived.
In practice instead of the pixel size, the dimensions and the horizontal and vertical resolution of displays are given, hence it is useful to express the minimal viewing distance as the function of these quantities.

In the following, the geometry depicted in Figure \ref{Fig:optimal_vd} is investigated in case of a display with given height $H$ and number of active lines $N_V$, with the display located at a distance $D$ from the viewer.
The vertical pixel size is then $\frac{H}{N_V}$.

The perceived angle between the adjacent pixels ate the observer position is then given as 
\begin{equation}
\tan \frac{\Phi}{2} = \frac{H}{2 N_V D}.
\end{equation}
For the sake of simplicity the small argument approximation of the tangent function can be applied, i.e. $\tan x \approx x$, ha $x \ll 1$, leading to
\begin{equation}
\Phi = \frac{H}{N_V D} \hspace{3mm} \rightarrow \hspace{3mm} D = \frac{H}{N_V \Phi}.
\end{equation}
By substituting the visual activity of the HVS ($\frac{1}{60}\cdot \frac{\pi}{180}~\mathrm{rad} = 2.9 \cdot 10^{-4}$) for a display with given vertical resolution the optimal (minimal) viewing distance is given by
\begin{equation}
D = H \frac{1}{N_V \,  2.9 \cdot 10^{-4}}.
\end{equation}
This is the so-called \textbf{Lechner distance}, formulating the optimal viewing distance, taken into consideration during the deigning of a display with given size and resolution.

\begin{table}[h!]
\caption{Optimal viewing distance of common SD and HD formats and the covered observer field of view}
\renewcommand*{\arraystretch}{2.25}
\label{tab:viewing_dist}
\begin{center}
    \begin{tabular}[h!]{ @{}c | | l | l | l @{} }%\toprule
				         &   SD, 480i & SD, 576i &	 HDTV \\ \hline
    Active lines:	 &     480 	  		   &   576   				&	 1080\\
    Viewing distance:   &  7 x height &  6 x height & 3 x height\\
    Viewing distance:       &  4.25 x diameter &  3.6 x diameter & 1.5 x diameter\\
    Horizontal field of view &  $\approx 11^{\circ}$ &    $\approx 13^{\circ}$ & $\approx 32^{\circ}$ \\
    \end{tabular}
\end{center}
\end{table}

In case that also the display aspect ratio (denoted by $a_r$) is given (for SD 4:3 and for HD 16:9) the Lechner distance can be expressed as the function of the horizontal dimension (for the sake of simplicity assuming square pixels):
\begin{equation}
D = \frac{W}{a_r} \frac{1}{N_V \,2.9 \cdot 10^{-4}},
\end{equation}
where $W$ is the horizontal dimension (width) of the display.
Furthermore, if the display is observed from the optimal viewing distance, the horizontal field of view angle can be expressed, included by the display:
\begin{equation}
\tan \frac{\Phi_H}{2} = \frac{W}{2 D} \hspace{1cm} \rightarrow \hspace{1cm} D = \frac{W}{2 \tan \frac{\Phi_H}{2}}, 
\end{equation}
and the field of view is written as
\begin{equation}
\Phi_H = 2\arctan \left( \frac{a_r \, N_V \, 2.9\cdot 10^{-4}}{2} \right).
\end{equation}

The evaluation of these results for the discussed SD formats and the upcoming HD formats is summarized in Table \ref{tab:viewing_dist}.
As a result it is obtained that displays with SD resolution should be observed from a distance 6-7 times the height of the screen.
In this case, it is verified that the display covers about 10-13 degrees from the observer's field of view horizontally.

The table already foreshadows the main motivation behind the introduction of HD video format: increasing the perceived reality of the reproduced scene, by filling an increased field of view with video content, compared to SDTV. Therefore, the basic goal was not to squeeze six times the number of pixels into the same visual angle opposed to the popular misbelief.
Instead the angular subtense of a single pixel should be maintained, and the entire image shoud occupy a much larger area of the viewer's visual field.

\subsubsection*{Short HD history}

Although today the term high definition format is associated with digital video, even in the era of early analog TV system increased resolution, high definition initiations existed.
Years before the introduction of color television, in 1949 France started analog monochromatic transmission with a high resolution standard at 819 lines (with 737 active lines), a system that should have been high definition even by today's standards, discontinued only in 1983. 
In 1958, the Soviet Union developed the Transfomator, the first high-resolution television system capable of producing an image composed of 1125 lines of resolution aimed at providing teleconferencing for military command. It was a research project and the system was never deployed by either the military or consumer broadcasting.

In 1979, the Japanese public broadcaster NHK (Japan Broadcasting Corporation) developed consumer high definition television with 1125 scan lines and a 5:3 display aspect ratio.
The system, known as Hi-Vision or MUSE required about twice the bandwidth of the existing NTSC system but provided about four times the resolution, applied for analog regular HD broadcasting beginning in 1994.
The limited standardization of analog HDTV in the 1990 did not lead to global HDTV adoption as technical and economic constraints did not permit HDTV to use bandwidths greater than normal television.

The introduction of the HD format was finally motivated by the emerging of digital compression methods (mainly of MPEG-1 and MPEG-2).
The HD format was codified in the \href{https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.709-6-201506-I!!PDF-E.pdf}{ITU-709} (Rec. 709) standard, published in 1990.

\subsubsection*{HD parameters}

The ITU-709 recommendation prescribed the following properties for HD video format:
\begin{itemize}
\item \textbf{Aspect ratio:} The first standardized parameter for the new video format was its aspect ration, being chosen to 16:9.
The choice is not obvious, since at the time of the standardization process no video content was captured with this aspect ratio: 
TV movies were shot with the aspect ratio of SD format (4:3), while movie films used widescreen formats (most commonly the 1.85:1 and 2.2:1 widescreen, or 2.35:1 anamorphic format).
It has been found that rectangles with the side ratios of the above popular aspect ratios and with equal areas exactly fit within an outer rectangle with the aspect ratio of 16:9.
Furthermore, the intersection of all these rectangles is an inner rectangle with the aspect ratio of 16:9.
Therefore, the aspect ratio of 16:9 ensured the compatibility with most of the then-existing formats.
The geometry is shown in Figure \ref{Fig:kerns_powers}.
%
\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.9 \columnwidth ]{Figures/KernsPowers.png}
	\small
	\end{overpic}
	\caption{The 16:9 aspect ratio as the outer and inner rectangle of rectangles with common aspect ratios}
	\label{Fig:kerns_powers}
\end{figure}
%
\item \textbf{Raster scan, field and frame rate:}
In the HD system besides interlaced scanning the possibility of progressive scan was introduced.
As the legacy of the SD system, numerous field rate and field rates are specified by the format, being $24~\mathrm{Hz}$, $25~\mathrm{Hz}$, $30~\mathrm{Hz}$, $50~\mathrm{Hz}$ and $60~\mathrm{Hz}$ field/frame rates, along with fractional rates, having the above values multiplied by $\frac{1000}{1001}$.


\item \textbf{Sampling frequency:}
The HD sampling frequency can be derived from the SD sampling rate:
One of the main goals of the HD format was to double both the horizontal ($\times 2$) and vertical ($\times 2$) spatial resolution, along with the aspect ratio changed to 16:9 ($4 : 3 \times 4/3$), which the total pixel number being at least $2\times 2 \times \frac{4}{3} = 5.33$ times that of the SD format.
In order to ensure orthogonal sampling (only full pixels in a line) for most field and frame rates the optimal choice for the sampling frequency was $5.5 \cdot 13.5 = 74.25~\mathrm{MHz}$ and the fractional sampling frequency $\frac{1000}{1001} \cdot 74.25~\mathrm{MHz}$ for fractional field/frame rates.
In case of progressive scanning for frame rates $50-60~\mathrm{Hz}$ the data rate is double compared to the interlaced case, therefore, a doubled sampling frequency of $fs = 148.5~\mathrm{MHz}$ is required.
%TODO : kibogarászni KovácsI jegyzetéből a mintavételi frek. megválasztását (VidStTech 01.pdf, 46.o)

\item \textbf{Spatial resolution:} 
After a lengthy debate \footnote{The first version of the standard specified 1035 active lines, which was later withdrawn.} the number of active line has been codified to 1080 and the total number of lines to 1125, including the vertical blanking with both progressive and interlaced scan modes (this raster format was also codified in SMPTE 274M).
The horizontal number of active pixels has been chosen to a fixed number of 1920 regardless of the frame/field rate, resulting in the active resolution of $1920 \times 1080$.
Therefore, the HD formats with different field and frame rates only differ in the number of total horizontal number of pixels (i.e. in the length of the horizontal blanking time).
%
\begin{table}[h!]
\caption{Sampling frequency, total number of lines and columns and active resolution of commonly used HD formats.
The total number of pixels in a line can be calculated according to $N_\mathrm{H} = f_s \times \frac{f_{\mathrm{V}}}{N_\mathrm{V}}$, with $f_{\mathrm{V}}$ being the frame rate.}
\renewcommand*{\arraystretch}{2.25}
\label{tab:viewing_dist}
\begin{center}
    \begin{tabular}[h!]{ @{}c | l | l | l | l | l @{} }%\toprule
		Format       &   $f_s \, [\mathrm{MHz}]$ 				& $N_\mathrm{H}$ & $N_\mathrm{V}$ & Active resolution \\ \hline
		$720p50$     &   $74.25~\mathrm{MHz}$    				& 1980     &  750  & $1280 \times 720$ \\
		$720p59.94$  &$74.25\cdot\frac{1000}{1001}~\mathrm{MHz}$& 1650     &  750  & $1280 \times 720$ \\
		$1080i25$ 	 &   $74.25~\mathrm{MHz}$    				& 2640     & 1125  & $1920 \times 1080$ \\
		$1080i30$  	 &   $74.25~\mathrm{MHz}$    				& 2200     & 1125  & $1920 \times 1080$ \\
		$1080p50$ 	 &   $148.5~\mathrm{MHz}$    		    	& 2640     & 1125  & $1920 \times 1080$ \\
		$1080p59.94$ &$148.5\cdot\frac{1000}{1001}~\mathrm{MHz}$& 2200     & 1125  & $1920 \times 1080$ 
        \end{tabular}
\end{center}
\end{table}
\vspace{3mm}
The different HD formats are denoted using the following convention:
\begin{itemize}
\item Active resolution in pixels, denoting often only the vertical resolution
\item Scanning mode: $p$ for progressive and $i$ for interlaced
\item frame rate.
For interlaced video often instead of frame frate---incorrectly---field rate is marked.
\end{itemize}
As an example: $1080i25$ denotes interlaced video with 1080 active lines, and the frame rate of $25~\mathrm{Hz}$ (i.e. field rate is $50~\mathrm{Hz}$).
\begin{figure}[]
	\centering
	\begin{overpic}[width = 1 \columnwidth ]{Figures/HD_format.png}
	\small
	\end{overpic}
	\caption{Az 1080 soros és 720 soros HD formátum szemléltetése}
	\label{Fig:HD_formats}
\end{figure}

Due to the large data rate of progressive HD formats the ITU-709 has been extended with a lower spatial resolution format, containing 720 active lines and 1280 active pixels in a line, with exclusively progressive scanning mode.
\footnote{
As already discussed, the basic goal of 1080 lined format was to double the number of scan lines.
The line number of the 720p format---as an intermediate format between SD and HD---applies $\frac{3}{2} \cdot 480 = 720$ scan lines, with the horizontal number of pixels obtained from the aspect ratio of 16:9}.
It is denoted by: \textbf{720p}.
Two examples for the two basic HD formats can be seen in \ref{Fig:HD_formats}.
\item The primaries of the HD format coincide with that of the ITU-601 SD format, resulting in the same gamut.
The luma (and the relative luminance) is calculated as
\begin{equation}
Y' = 0.2126 R' + 0.7152 G' + 0.0722B'.
\end{equation}
\begin{figure}[]
	\centering
	\begin{overpic}[width = 1\columnwidth ]{Figures/hd_line.png}
	\end{overpic} 
	\caption{Structure of one line of the 1080 lined HD format.}
	\label{Fig:hd_line}
\end{figure}
Opposed to the SD formats, in this case the luma coefficients are mathematically correct, obtained from the relative luminance coefficients of the chosen primaries and its white point, i.e. the coefficients correspond to the second row of the $RGB \hspace{3mm} \rightarrow \hspace{3mm} XYZ$ transformation matrix.
\item The opto-electronic transfer function coincides with the SD's OETF:
\begin{equation}
E = 
\begin{cases}
4.500 L, \hspace{20mm} \mathrm{ha}\, L < 0.018 \\
1.099 L^{0.45} - 0.099, \hspace{3mm} \mathrm{ha}\, L \geq 0.018,
\end{cases}
\end{equation}
where $L \in \{ R, G, B \}$.
\item The bit depth is 8 bits for consumer and 10 bits for studio applications.
\item The studio standard codifies the chroma subsampling scheme of 4:2:2.
\end{itemize}
The structure of the resulting standardized HD video signal is the same as the SD video signal, discussed in the foregoing, illustrated in Figure \ref{Fig:hd_line}.
The sole difference is the application of tri-state sync pulses, with the physical dynamic range usually being $0, \pm300~\mathrm{mV}$.

\subsubsection*{Uncompressed data rate of HD formats}

In the following the uncompressed data rate of different video formats is discussed.
With denoting the active and total pixel dimensions the notations showed in Figure \ref{Fig:size} the total data rate can be calculated as
\begin{equation}
B\!R_T = \underbrace{N_{\mathrm{H}} \cdot N_{\mathrm{V}} \cdot f_{\mathrm{V}} \cdot n_{\mathrm{bit}}}_{\text{bitrate per sample}} \cdot n_{\mathrm{CS}},
\end{equation}
with denoting $f_{V}$ the frame rate, $n_{\mathrm{bit}}$ the bit depth and $n_{\mathrm{CS}}$ the number of components, depending on the chroma subsampling scheme applied.
The value of the latter is $n_{\mathrm{CS}} = 3$ for 4:4:4, $n_{\mathrm{CS}} = 2$ for 4:2:2 and $n_{\mathrm{CS}} = 1.5$ for 4:2:0 schemes.

Similarly the active data rate is calculated as
\begin{equation}
B\!R_A = N_{\mathrm{H,A}} \cdot N_{\mathrm{V,A}} \cdot f_f \cdot n_{\mathrm{bit}} \cdot n_{\mathrm{CS}}
\end{equation}.

The active and total bit rates of several frequently used video formats are summarized in Table \ref{tab:bitrate}\footnote{
The vertical blanking interval of UHD format is \href{http://programmersought.com/article/7908103552/}{fixed} to 90 lines, with the horizontal blanking depending on the frame rate of the video.
For a video with the frame rate of $60~\mathrm{Hz}$ the horizontal blanking interval is 560 samples.
}.
\begin{figure}[]
\captionsetup{singlelinecheck=off}
	\centering
	\begin{minipage}[c]{0.4\textwidth}
	\begin{overpic}[width = 1\columnwidth ]{Figures/size.png}
	\end{overpic}   \end{minipage}\hfill
		\begin{minipage}[c]{0.55\textwidth}
	\caption[]{Notation-convention for calculating the data rate of video
	\begin{itemize}
	\item $N_{\mathrm{H}}$: Total pixel number/line
	\item $N_{\mathrm{H,A}}$: Active pixel number/line
	\item $N_\mathrm{V}$: Total number of lines/frame
	\item $N_\mathrm{V,A}$: Number of active lines/frame
	\end{itemize}}
	\label{Fig:size}  \end{minipage}
\end{figure}
%
\begin{table}[h!]
\caption{The total and active bitrate of frequently used video formats}
\renewcommand*{\arraystretch}{2.25}
\label{tab:bitratet}
\begin{center}
    \begin{tabular}[h!]{ @{}c | l | l | l | l | l @{} }%\toprule
\thead{Format} & \thead{Sampling \\ frequency} &    \thead{Total bitrate \\ 4:2:2} & \thead{Active bitrate\\ 4:2:2} & \thead{Active  bitrate\\ 4:4:4} \\ \hline
$576p50$     &   $13.5~\mathrm{MHz}$        & $0.54~\mathrm{Gbit/s}$  & $0.41~\mathrm{Gbit/s}$ & $0.62~\mathrm{Gbit/s}$ \\
$720p60$     &   $74.25~\mathrm{MHz}$    	& $1.49~\mathrm{Gbit/s}$  & $1.11~\mathrm{Gbit/s}$ & $1.67~\mathrm{Gbit/s}$ \\
$1080i30$ 	 &   $74.25~\mathrm{MHz}$    	& $1.49~\mathrm{Gbit/s}$  & $1.24~\mathrm{Gbit/s}$ & $1.86~\mathrm{Gbit/s}$ \\
$1080p60$ 	 &   $148.5~\mathrm{MHz}$    	& $2.97~\mathrm{Gbit/s}$  & $2.49~\mathrm{Gbit/s}$ & $3.73~\mathrm{Gbit/s}$ \\
$2160p60$ 	 &   $297~\mathrm{MHz}$         &  $11.88~\mathrm{Gbit/s}$& $9.96~\mathrm{Gbit/s}$ & $14.93~\mathrm{Gbit/s}$
\end{tabular}
\end{center}
\end{table}
%
The uncompressed bit rate approximately equals for $720p$ and $1080i$ formats.
An advantage of the progressive format is it can be more efficiently compressed than interlaced video.
On the other hand interlaced format ensures high vertical resolution for still and slowly moving video content, although with the resolution degrading for moving reproduced objects.
Therefore, broadcasting operators with sport content in their main profile traditionally chose $720p50/60$ format, while operators, broadcasting mainly news and movies usually apply $1080i$ video format.

%TODO 8b/10b codin
% Fischer Digital Video and Audio broadcasting technology 227.o

%TODO HD ready
%TODO Gaumt coverage percentage, Surface colors
%TODO In typical production practice the encoding function of image sources is adjusted so that the final picture has the desired aesthetic look, as viewed on a reference monitor with a gamma of 2.4 (per ITU-R BT.1886) in a dim reference viewing environment (per ITU-R BT.2035).[10][11][12]
%TODO Rec. 2100, a standard for HDTV and UHDTV with high dynamic range

\subsection{The UHD format}

The original intention behind the introduction of HD format was to enhance the visual experience by increasing the listener's visual angle filled with video content.
The ultra high definition format aims at the further improvement of the reproduction quality by applying even larger display sizes---covering an even larger part of the field of view---and by increasing the displayed image quality.

Similarly to the HD format, the first notable initiation of UHD technology is connected to the Japanese NHK, capturing video data at the resolution of $7680 \times 4096$ by using an array of 16 HDTV recorders and four CCD sensors with the resolution $3840 \times 2048$, as early as 2003.
The first UHD standard was published in 2007 (SMPTE 2036), while the currently accepted UHD codification was introduced in 2012, entitled the \textbf{ITU-R BT. 2020}.
The standard specifies two UHD formats, the 4k with its spatial resolution being the double of the 1080p format both horizontally and vertically (meaning 4 times larger total pixel size), and the 8k, with a further  doubled resolution, compared to 4k.

\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.85 \columnwidth ]{Figures/fov_formats.png}
	\small
	\end{overpic}
	\caption{Visual angle ensured by SD, HD and UHD formats, with the screen being watched from the optimal viewing distance.}
	\label{Fig:fov_formats}
\end{figure}

Theoretically, the goal of the 4k and 8k formats was to fill the visual angle of approximately $58^{\circ}$ and $96^{\circ}$ with content, respectively.
The visual angle of the different SD, HD and UHD formats are compared in Figure \ref{Fig:fov_formats}.

In order to ensure high quality video reproduction over a large visual angle the ITU-2020 improved the reproduction parameters in numerous aspect compared to HD:
\begin{itemize}
\item \textbf{Spatial resolution:} 
The standard specifies two resolution types: $3840 \times 2160$ termed as 4k and $7680 \times 4320$ as 8k resolutions.
Both 4k and 8k employs squared pixels, i.e. both the display and storage aspect ratios are 16:9.
%
\item \textbf{Scanning type:}
Unlike HD, ITU-2020 exclusively allows progressive scanning mode (i.e. the $p/i$ designation is no longer used).

\item \textbf{Frame rate:}
With the increased visual angle the UHD content already fills the peripheral vision of the observer with content.
Since the peripheral vision is dominated by rods with a much quicker response time than the cones of the central vision, thus, in order to ensure continuous motion and avoid flickering significantly higher frame rates are supported than the HD frame rates.
The standard allows the frame rates of $120, 119.88, 100, 60, 59.94, 50, 30, 29.97, 25, 24, 23.976~\mathrm{Hz}$.
\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.75 \columnwidth ]{Figures/uhd_gamut.png}
	\small
	\end{overpic}
	\caption{Gamut of the ITU-2020 UHD color space compared with the gamut of SD and HD color representations.}
	\label{Fig:UHD_gamut}
\end{figure}

\item \textbf{Color space:} 
For the first time since the introduction of NTSC the ITU-2020 UHD standard applies new RGB primaries for color representation for the sake of an enlarged gamut.
The resulting gamut is shown in Figure \ref{Fig:UHD_gamut}: 
As the figure verifies it, the UHD color space applies spectral colors for the RGB primaries, described by the wavelengths of $\lambda_R = 630~\mathrm{nm}$, $\lambda_G =532~\mathrm{nm}$ and $\lambda_B =467~\mathrm{nm}$.
\footnote{
Obviously, this does not mean that UHD displays use spectral colors as RGB primaries, instead the color pixels of UHD video content are stored and transmitted in terms of spectral primary colors.
Monitors and TVs display colors by using the actual applied LCD or LED primary colors, first converting the ITU-2020 content into the RGB color space of the display.
The gamut of these color spaces are obviously smaller than the gamut of the UHD standard: as an example, JDI introduced a \href{https://www.displaydaily.com/?view=article&id=62235:jdi-may-have-commercial-problems-but-has-technical-highlights}{studio reference monitor} with the diameter of $14.3''$ in 2018, allowing the reproduction of $97\%$ of the ITU-2020 color space.}.
The ITU-2020 covers the $75.8\% of$ the CIE chromaticity diagram and the entire Pointer's gamut of real surface colors
\footnote{
The Pointer's gamut is the result of the series of measurements, containing the chromaticites (hues) of colors of reflective surfaces, occurring in the natural environment---opposed to the colors that can be produced by emissive surfaces, e.g. neon or colors of LED/LCD light sources.
The \href{https://cinepedia.com/picture/color-gamut/}{Pointer's gamut} is based on 4089 measurement samples, with the database published in 1980.
Since then, the Pointer's gamut is a de facto standard of qualifying \href{https://www.tftcentral.co.uk/articles/pointers_gamut.htm}{color spaces}.}.
The relative luminance (and the luma) can be calculated from the RGB coordinates as 
\begin{equation}
Y = 0.2627 \, R + 0.6780 \, G + 0.0593 \, B.
\label{eq:2020_Y}
\end{equation}
\item \textbf{Bit depth:} 
The increase of the range of the described colors requires increasing the bit depth of representation as well, in order to ensure the same quantization precision.
Since a larger color space increases the difference between colors an increase of 1-bit per sample is needed for Rec. 2020 to equal or exceed the color precision of ITU-709.
Thus, Rec. 2020 defines a bit depth of either 10 bits per sample for consumer or 12 bits per sample for studio applications.
\item \textbf{The opto-electronic transfer function:} 
The OETF of the ITU-2020 coincides with that of the SD and HD standards:
\begin{equation}
E = 
\begin{cases}
4.500 L, \hspace{20mm} \mathrm{ha}\, L < \beta \\
\alpha L^{0.45} - (\alpha - 1 ), \hspace{3mm} \mathrm{ha}\, L \geq \beta,
\end{cases}
\end{equation}
where $\alpha = 1.09929682680944$ and $\beta = 0.018053968510807$.
The only difference is the precision of the coefficients: for 12 bits bit depth the above coefficients should be evaluated with 5 digits precision.
%
\item \textbf{Chroma subsampling:} 
The standard specifies the subsampling schemes of 4:2:0, 4:2:2 and 4:4:4.
In the latter case instead of luma-chroma representation direct $R'G'B'$ representation is allows.
In case of 4:2:0 or 4:2:2 sampling, besides \ycbcr representation the so-called \textbf{contant luminance mode} is supported, resulting in the $Y C_{\mathrm{bc}} C_{\mathrm{rc}}$ components.
This constant luminance mode may be used when the top priority is the most accurate retention of luminance information.
The luma component in $Y C_{\mathrm{bc}} C_{\mathrm{rc}}$ is calculated using the same coefficient values as for \ycbcr, but it is calculated from linear RGB and then gamma corrected, rather than being calculated from gamma-corrected $R'G'B'$.
As a result $Y'$ in constant luminance mode is mathematically accurately describes the gamma-corrected relative luminance component (therefore, the resulting luma contains no color information and the resulting chroma contains no luminance information).
The prevents artifacts that arise in case of \ycbcr representation due to the subsampling of minor luminance information, present in the chroma components.
\end{itemize}

%TODO \paragraph{Konstans fénysűrűségű mód:\\}
\subsection{Digital interfaces}
The most common physical interface for SD, HD and UHD video transmission in video studiotechnologies is the SDI (Serial Digital Interface), and HDMI (High-Definition Multimedia Interface) in consumer applications.

The bit rates of most common video formats were summarized in Table \ref{tab:bitratet}.
As a comparison, the maximal digital bandwidth of the different HDMI interface versions are the following
\begin{itemize}
\item HDMI 1.0-1.2: 4.95 Gbit/s (3.96 Gbit/s effective)\footnote{
Due to undiscussed reasons the HDMI interface applies a so-called 8b/10b channel coding, transmitting 8 bits of data in 10 bit sequences.
Therefore, only the part of $\frac{8}{10}$ of the total bandwith can used for effective data transmission.}
\item HDMI 2.0: 18 Gbit/s (14.4 Gbit/s effective)
\item HDMI 2.1: 48 Gbit/s (38.4 Gbit/s effective)
\end{itemize}
Obviously, in order to find the HDMI version for the transmission of a given video format, the total number of lines and sample per lines have to be taken into consideration, since HDMI signals contains the horizontal and vertical blanking intervals as well.
As discussed earlier: the vertical blanking interval carries multichannel audio streams and other auxiliary data, presented simultaneously with the video data.

It can be concluded that HDMI 1.0 version was created mainly for the transmission of 1080p video format with 4:2:2 chroma subsampling scheme, and the bit depth of 10 or 12 bits (4:4:4 video can be only transmitted with this version represented on 8 bits).
On the other hand, HDMI 2.0 was developed for 4k, and the 2.1 version for 8k video transmission.

\begin{figure}[]
	\centering
	\begin{overpic}[width = 0.75 \columnwidth ]{Figures/optimal-viewing-distance-television-graph-size.png}
	\small
	\end{overpic}
	\caption{Optimal viewing distance as the function of display diameter.}
	\label{Fig:optimal_vd_2}
\end{figure}

\subsection{Optimal choice of display resolution}

As the conclusion of the present chapter the Lechner distance is revisited in order to arrive at the the optimal viewing distance for a given display size and for the optimal display size for a given observer distance.
Besides the Lechner distance, several other recommendations exist for the optimum viewing distance, including manufacturers, retail and THX recommendations.
\begin{itemize}
\item SMPTE 30: is the standardized codification of the Lechner distance, recommending the viewing distance of 1.6 times the diameter in case of a HD display with 1080 lines, resulting in a field of view of $30^{\circ}$.
This recommendation is very popular with the home theater enthusiast community, appearing in books on home theater design,
%
\item The recommendation of manufacturers, retails and several publications suggests the viewing distance of 2.5 times the diameter in case of a HD display, resulting in the field of view of $20^{\circ}$.
%
\item THX recommends that the ,,best seat-to-screen distance'' is one where the view angle approximates $40^{\circ}$, which according to the THX approximates the cinema experience the most.
This is achieved in case of HD format with the viewing distance being 1.2 times the display diameter.
\end{itemize}
Although there are slight discrepancies between these recommendations, they all agree in that for the viewing distance of the HD displays ,,the closer the better''.

The graph of optimal viewing distances can be visualized for the different video formats, as depicted in Figure \ref{Fig:optimal_vd_2}.
At a given display size, of course the viewing distance can be increased, the pixel structure of the display does not become visible.
Thus, at a fixed display size above a given line in the graph the display is applicable.
The graph, therefore, can be divided into regions, indicating the optimal display resolution for a given viewing distance and display size.

\vspace{3mm}
Based on statistics, conducted by Bernard J. Lechner the average domestic TV viewing distance is approximately $2.7~\mathrm{m}$.
At this distance displays above the diameter of 50'' should have the resolution of 1080p, while in order to exploit the advantages and quality of 4k resolution the display should have a diameter of at least 75'' ($\sim 1.9~\mathrm{m}$).
Displays with the diameter of 2 meters are extremely rarely applied in domestic use even today, suggesting that the capabilities of even 4k displays are not completely utilized nowadays.
However, the introduction of consumer 8k displays is already trending, with also experimental 8k broadcasting initiations beginning in the recent years.
As an example, the first dedicated satellite for broadcasting 8k content has been lunched (BSAT-4a), which was planned to broadcast the 2020 summer olympics, which has been however postponed to 2021 due to the outbreak of the COVID-19 pandemic.

%Periférikus látás:
%https://www.quora.com/What-is-the-aspect-ratio-of-human-vision

%TODO \subsection{A HDR kiterjesztés}

\vspace{2cm}
\noindent\rule{12cm}{0.4pt}

\subsection*{End-of-Chapter Questions}

\begin{itemize}
\item What were the reasons behind the introduction of the interlaced format?
What is the main idea of interlaced scanning?
%
\item How was the sampling frequency of the SD format chosen?
How was it extended in order to choose the sampling frequency of the HD format?
%
\item Calculate the optimal viewing distance for a 4k (2160p) display with the diameter of 65'' (the aspect ratio is 16:9)!
%
\item List some improvements of the UHD standard compared to the HD format!
%
\item Define the number of total and active resolution of the $2160p60$ format!
The number of inactive lines is 90 and the sampling frequency is $297~\mathrm{MHz}$.
%
\item Define the total bitrate of the video format of the previous example in case of a chroma subsampling scheme of 4:2:2, with 12 bits per sample representation!
Pick the minimal HDMI version which is capable of transmitting the exemplary video stream, if the HDMI transmits 8 bit data in 10 bits sequences (i.e. the effective bandwidth is $\frac{8}{10}$ times the total bandwidth), and the total bandwidth of the HDMI interface versions are
\begin{itemize}
\item HDMI 1.0-1.2: $4.95~\mathrm{Gbit/s}$
\item HDMI 1.3-1.4: $10.2~\mathrm{Gbit/s}$
\item HDMI 2.0-1.2: $18~\mathrm{Gbit/s}$
\item HDMI 2.1: $48~\mathrm{Gbit/s}$
\end{itemize}
\end{itemize}